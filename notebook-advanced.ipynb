{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IT3212"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eda = False\n",
    "lemmatize = False\n",
    "with_sentiment = False\n",
    "\n",
    "text_embedding = {\n",
    "    'tfidf': True,\n",
    "    'word2vec': False,\n",
    "    'bert': False,\n",
    "}\n",
    "\n",
    "hyperparameter_tuning = {\n",
    "  'adaboost': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# NLTK tools and datasets\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Uncomment if you need to download NLTK data packages\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "# Text processing\n",
    "from textblob import TextBlob\n",
    "import contractions\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, classification_report, \n",
    "                             confusion_matrix, roc_curve, auc)\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Miscellaneous\n",
    "from collections import Counter\n",
    "from urllib.parse import unquote\n",
    "from scipy import stats\n",
    "import chardet\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix dataset encoding issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some rows in the raw data include non UTF-8 characters. \n",
    "\n",
    "# Example of text with non UTF-8 characters:\n",
    "# 778245336,FALSE,finalized,5,8/30/15 13:27,Not Relevant,0.7952,,army,\n",
    "# text column: Pakistan,\".: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: RT DrAyesha4: #IndiaKoMunTorJawabDo Indian Army ki��_ http://t.co/WJLJq3yA4g\"\n",
    "# ,6.29079E+17,195397186\n",
    "\n",
    "# Chardet identifies the encoding of the raw data as 'MacRoman'.\n",
    "# For now, we will remove all non UTF-8 characters from the raw data\n",
    "# We handle this by removing all � characters from the raw data and writing the modified content back to the file.\n",
    "\n",
    "def fix_non_utf8_encoding(filepath, destination_filepath):\n",
    "    with open(filepath, 'rb') as file:\n",
    "        rawdata = file.read()\n",
    "        result = chardet.detect(rawdata)\n",
    "        print(result['encoding'])\n",
    "\n",
    "\n",
    "    # Open the file in read mode, read its contents, then close it\n",
    "    with open('data/disaster-tweets.csv', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Remove all � characters\n",
    "    content = content.replace('�', '')\n",
    "\n",
    "    # Open the file in write mode and write the modified content back to it\n",
    "    with open(destination_filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "filepath = 'data/disaster-tweets.csv'\n",
    "dest = 'data/disaster-tweets-utf8.csv'\n",
    "\n",
    "# fix_non_utf8_encoding(filepath, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(filepath):\n",
    "    df = pd.read_csv(filepath, encoding='utf-8')\n",
    "    train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "    return train_data, test_data\n",
    "\n",
    "filepath = 'data/disaster-tweets-utf8.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>choose_one:confidence</th>\n",
       "      <th>choose_one_gold</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>778253309</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 16:07</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>screamed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i dont even remember slsp happening i just remember being like wtf and then the lights turned off and everyone screamed for the encore</td>\n",
       "      <td>6.291070e+17</td>\n",
       "      <td>2.327739e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>778251995</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 20:16</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mudslide</td>\n",
       "      <td>Edinburgh</td>\n",
       "      <td>@hazelannmac ooh now I feel guilty about wishing hatman out. I bet the mudslide was delicious!</td>\n",
       "      <td>6.290180e+17</td>\n",
       "      <td>2.750220e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>778247239</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/30/15 0:15</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>collide</td>\n",
       "      <td>planeta H2o</td>\n",
       "      <td>Soultech - Collide (Club Mix) http://t.co/8xIxBsPOT8</td>\n",
       "      <td>6.290920e+17</td>\n",
       "      <td>6.052387e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>778255430</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 17:03</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.7978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wounded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police Officer Wounded Suspect Dead After Exchanging Shots - http://t.co/iPHaZV47g7</td>\n",
       "      <td>6.291190e+17</td>\n",
       "      <td>2.305930e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778255609</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 22:11</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Sunny Southern California</td>\n",
       "      <td>Cramer: Iger's 3 words that wrecked Disney's stock http://t.co/4dGpBAiVL7</td>\n",
       "      <td>6.290800e+17</td>\n",
       "      <td>2.464266e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  778253309    False   finalized                   5     8/27/15 16:07   \n",
       "1  778251995    False   finalized                   5     8/27/15 20:16   \n",
       "2  778247239    False   finalized                   5      8/30/15 0:15   \n",
       "3  778255430    False   finalized                   5     8/27/15 17:03   \n",
       "4  778255609    False   finalized                   5     8/27/15 22:11   \n",
       "\n",
       "     choose_one  choose_one:confidence choose_one_gold   keyword  \\\n",
       "0  Not Relevant                 1.0000             NaN  screamed   \n",
       "1  Not Relevant                 1.0000             NaN  mudslide   \n",
       "2  Not Relevant                 1.0000             NaN   collide   \n",
       "3      Relevant                 0.7978             NaN   wounded   \n",
       "4  Not Relevant                 1.0000             NaN   wrecked   \n",
       "\n",
       "                    location  \\\n",
       "0                        NaN   \n",
       "1                  Edinburgh   \n",
       "2                planeta H2o   \n",
       "3                        NaN   \n",
       "4  Sunny Southern California   \n",
       "\n",
       "                                                                                                                                     text  \\\n",
       "0  i dont even remember slsp happening i just remember being like wtf and then the lights turned off and everyone screamed for the encore   \n",
       "1                                          @hazelannmac ooh now I feel guilty about wishing hatman out. I bet the mudslide was delicious!   \n",
       "2                                                                                    Soultech - Collide (Club Mix) http://t.co/8xIxBsPOT8   \n",
       "3                                                     Police Officer Wounded Suspect Dead After Exchanging Shots - http://t.co/iPHaZV47g7   \n",
       "4                                                               Cramer: Iger's 3 words that wrecked Disney's stock http://t.co/4dGpBAiVL7   \n",
       "\n",
       "        tweetid        userid  \n",
       "0  6.291070e+17  2.327739e+08  \n",
       "1  6.290180e+17  2.750220e+07  \n",
       "2  6.290920e+17  6.052387e+08  \n",
       "3  6.291190e+17  2.305930e+09  \n",
       "4  6.290800e+17  2.464266e+07  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import_remote = False\n",
    "\n",
    "if import_remote:\n",
    "    df_train = pd.read_csv('https://raw.githubusercontent.com/magnusrodseth/it3212/main/data/train.csv', encoding='utf-8')\n",
    "    df_test = pd.read_csv('https://raw.githubusercontent.com/magnusrodseth/it3212/main/data/test.csv', encoding='utf-8')\n",
    "else:\n",
    "    df_train, df_test = split_train_test(filepath)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    # Clean `keyword` column.\n",
    "\n",
    "    # Write the updated dataframe to a new CSV file\n",
    "    # Plot the most common keywords\n",
    "    defined_keywords = df_train[df_train['keyword'] != '']['keyword']\n",
    "\n",
    "    plt.figure()\n",
    "    sns.countplot(y=defined_keywords, order=defined_keywords.value_counts().iloc[:10].index)\n",
    "    plt.title('Most Common Keywords')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Keyword')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    # Compare keywords for disaster tweets and non-disaster tweets\n",
    "    disaster_keywords = df_train[df_train['choose_one'] == 'Relevant']['keyword']\n",
    "    non_disaster_keywords = df_train[df_train['choose_one'] == 'Not Relevant']['keyword']\n",
    "\n",
    "    # Create a figure object and define the grid\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))  # 1 row, 2 columns\n",
    "\n",
    "    # Plotting\n",
    "    sns.countplot(y=disaster_keywords, ax=ax[0], order=disaster_keywords.value_counts().iloc[:10].index, color='red')\n",
    "    sns.countplot(y=non_disaster_keywords, ax=ax[1], order=non_disaster_keywords.value_counts().iloc[:10].index, color='blue')\n",
    "\n",
    "    # Titles and labels\n",
    "    ax[0].set_title('Most Common Keywords for Disaster Tweets')\n",
    "    ax[0].set_xlabel('Count')\n",
    "    ax[0].set_ylabel('Keyword')\n",
    "\n",
    "    ax[1].set_title('Most Common Keywords for Non-Disaster Tweets')\n",
    "    ax[1].set_xlabel('Count')\n",
    "    ax[1].set_ylabel('Keyword')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, we can see that the top 10 shared keywords of disaster-related tweets and non-disaster-related tweets do not share any common keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 2167 of total: 8700 rows. Remaining rows: 6533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 635 duplicated rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sunset looked like erupting volcano initial thought pixar short lava</td>\n",
       "      <td>volcano</td>\n",
       "      <td>The sunset looked like an erupting volcano .... My initial thought was the Pixar short Lava http://t.co/g4sChqFEsT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7294 nikon d50 61 mp digital slr camera body 2 batteries carry bag charger 20000</td>\n",
       "      <td>body bag</td>\n",
       "      <td>#7294 Nikon D50 6.1 MP Digital SLR Camera Body 2 batteries carry bag and charger http://t.co/SL7PHqSGKV\\n\\n$200.00\\n_ http://t.co/T4Qh2OM8Op</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>mentaltwitter note make sure smoke alarm battery snuff times face many twitter reminders changing battery</td>\n",
       "      <td>smoke</td>\n",
       "      <td>Mental/Twitter Note: Make sure my smoke alarm battery is up to snuff at all times or face many twitter reminders of changing my battery.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>emergency need part 2 3 nashnewvideo nashgrier 103</td>\n",
       "      <td>emergency</td>\n",
       "      <td>?????? EMERGENCY ?????? NEED PART 2 and 3!!! #NashNewVideo http://t.co/TwdnNaIOns @Nashgrier 103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>whelen model 295ss100 siren amplifier police emergency vehicle full read ebay</td>\n",
       "      <td>siren</td>\n",
       "      <td>WHELEN MODEL 295SS-100 SIREN AMPLIFIER POLICE EMERGENCY VEHICLE - Full read by eBay http://t.co/Q3yYQi4A27 http://t.co/whEreofYAx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  \\\n",
       "0       1   \n",
       "1       1   \n",
       "2       0   \n",
       "3       0   \n",
       "4       0   \n",
       "\n",
       "                                                                                                        text  \\\n",
       "0                                       sunset looked like erupting volcano initial thought pixar short lava   \n",
       "1                           7294 nikon d50 61 mp digital slr camera body 2 batteries carry bag charger 20000   \n",
       "2  mentaltwitter note make sure smoke alarm battery snuff times face many twitter reminders changing battery   \n",
       "3                                                         emergency need part 2 3 nashnewvideo nashgrier 103   \n",
       "4                              whelen model 295ss100 siren amplifier police emergency vehicle full read ebay   \n",
       "\n",
       "     keyword  \\\n",
       "0    volcano   \n",
       "1   body bag   \n",
       "2      smoke   \n",
       "3  emergency   \n",
       "4      siren   \n",
       "\n",
       "                                                                                                                                       text_raw  \n",
       "0                            The sunset looked like an erupting volcano .... My initial thought was the Pixar short Lava http://t.co/g4sChqFEsT  \n",
       "1  #7294 Nikon D50 6.1 MP Digital SLR Camera Body 2 batteries carry bag and charger http://t.co/SL7PHqSGKV\\n\\n$200.00\\n_ http://t.co/T4Qh2OM8Op  \n",
       "2      Mental/Twitter Note: Make sure my smoke alarm battery is up to snuff at all times or face many twitter reminders of changing my battery.  \n",
       "3                                              ?????? EMERGENCY ?????? NEED PART 2 and 3!!! #NashNewVideo http://t.co/TwdnNaIOns @Nashgrier 103  \n",
       "4             WHELEN MODEL 295SS-100 SIREN AMPLIFIER POLICE EMERGENCY VEHICLE - Full read by eBay http://t.co/Q3yYQi4A27 http://t.co/whEreofYAx  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def filter_rows_by_confidence_and_decision(df, confidence_threshold):\n",
    "    df = df[df['choose_one:confidence'] >= confidence_threshold]\n",
    "    df = df[df['choose_one'] != \"Can't Decide\"]\n",
    "    return df\n",
    "\n",
    "def map_choose_one_to_y(df):\n",
    "    df['target'] = df['choose_one'].apply(lambda choice: 1 if choice == 'Relevant' else 0)\n",
    "    return df\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words(\"english\")])\n",
    "    if lemmatize:\n",
    "        text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    text = contractions.fix(text)\n",
    "    text = ' '.join(tokenizer.tokenize(text))\n",
    "    return text\n",
    "\n",
    "def clean_keyword(keyword):\n",
    "    return unquote(keyword) if pd.notnull(keyword) else ''\n",
    "\n",
    "def clean_data(df):\n",
    "    df['keyword'] = df['keyword'].apply(clean_keyword).apply(str.lower)\n",
    "    df['text_raw'] = df['text']\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "initial_count = df_train.shape[0]\n",
    "confidence_threshold = 0.7\n",
    "\n",
    "df_train = filter_rows_by_confidence_and_decision(df_train, confidence_threshold)\n",
    "print(\"Removed {} of total: {} rows. Remaining rows: {}\".format(initial_count - df_train.shape[0], initial_count, df_train.shape[0]))\n",
    "\n",
    "features_to_keep = ['target', 'text', 'keyword']\n",
    "\n",
    "df_train = map_choose_one_to_y(df_train)\n",
    "df_train = df_train[features_to_keep]\n",
    "df_train = clean_data(df_train)\n",
    "\n",
    "count_initial = df_train.shape[0]\n",
    "df_train = df_train.drop_duplicates(subset=['text'])\n",
    "print(\"Removed {} duplicated rows.\".format(count_initial - df_train.shape[0]))\n",
    "\n",
    "\n",
    "# Preprocess the test data as well\n",
    "df_test = map_choose_one_to_y(df_test)\n",
    "df_test = df_test[features_to_keep]\n",
    "df_test = clean_data(df_test)\n",
    "\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellanous features from `text` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text_raw</th>\n",
       "      <th>text_length</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>has_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>do not even remember slsp happening remember like wtf lights turned everyone screamed encore</td>\n",
       "      <td>screamed</td>\n",
       "      <td>i dont even remember slsp happening i just remember being like wtf and then the lights turned off and everyone screamed for the encore</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>hazelannmac ooh feel guilty wishing hatman bet mudslide delicious</td>\n",
       "      <td>mudslide</td>\n",
       "      <td>@hazelannmac ooh now I feel guilty about wishing hatman out. I bet the mudslide was delicious!</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>soultech collide club mix</td>\n",
       "      <td>collide</td>\n",
       "      <td>Soultech - Collide (Club Mix) http://t.co/8xIxBsPOT8</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>police officer wounded suspect dead exchanging shots</td>\n",
       "      <td>wounded</td>\n",
       "      <td>Police Officer Wounded Suspect Dead After Exchanging Shots - http://t.co/iPHaZV47g7</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>cramer igers 3 words wrecked disneys stock</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Cramer: Iger's 3 words that wrecked Disney's stock http://t.co/4dGpBAiVL7</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  \\\n",
       "0       0   \n",
       "1       0   \n",
       "2       0   \n",
       "3       1   \n",
       "4       0   \n",
       "\n",
       "                                                                                           text  \\\n",
       "0  do not even remember slsp happening remember like wtf lights turned everyone screamed encore   \n",
       "1                             hazelannmac ooh feel guilty wishing hatman bet mudslide delicious   \n",
       "2                                                                     soultech collide club mix   \n",
       "3                                          police officer wounded suspect dead exchanging shots   \n",
       "4                                                    cramer igers 3 words wrecked disneys stock   \n",
       "\n",
       "    keyword  \\\n",
       "0  screamed   \n",
       "1  mudslide   \n",
       "2   collide   \n",
       "3   wounded   \n",
       "4   wrecked   \n",
       "\n",
       "                                                                                                                                 text_raw  \\\n",
       "0  i dont even remember slsp happening i just remember being like wtf and then the lights turned off and everyone screamed for the encore   \n",
       "1                                          @hazelannmac ooh now I feel guilty about wishing hatman out. I bet the mudslide was delicious!   \n",
       "2                                                                                    Soultech - Collide (Club Mix) http://t.co/8xIxBsPOT8   \n",
       "3                                                     Police Officer Wounded Suspect Dead After Exchanging Shots - http://t.co/iPHaZV47g7   \n",
       "4                                                               Cramer: Iger's 3 words that wrecked Disney's stock http://t.co/4dGpBAiVL7   \n",
       "\n",
       "   text_length  hashtag_count  mention_count  has_url  \n",
       "0          134              0              0        0  \n",
       "1           94              0              1        0  \n",
       "2           52              0              0        1  \n",
       "3           83              0              0        1  \n",
       "4           73              0              0        1  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features(df): \n",
    "    # Create new column for text length\n",
    "    df['text_length'] = df['text_raw'].apply(len)\n",
    "    # Extract the number of hashtags\n",
    "    df[\"hashtag_count\"] = df[\"text_raw\"].apply(lambda x: len([c for c in str(x) if c == \"#\"]))\n",
    "\n",
    "    # Extract the number of mentions\n",
    "    df[\"mention_count\"] = df[\"text_raw\"].apply(lambda x: len([c for c in str(x) if c == \"@\"]))\n",
    "\n",
    "    # Extract the `has_url` feature\n",
    "    df[\"has_url\"] = df[\"text_raw\"].apply(lambda x: 1 if \"http\" in str(x) else 0)\n",
    "    return df\n",
    "\n",
    "# Write the updated dataframe to a CSV file\n",
    "df_train = extract_features(df_train)\n",
    "df_test = extract_features(df_test)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_with_ngrams</th>\n",
       "      <th>text_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do not even remember slsp happening remember like wtf lights turned everyone screamed encore do_not not_even even_remember remember_slsp slsp_happening happening_remember remember_like like_wtf wtf_lights lights_turned turned_everyone everyone_screamed screamed_encore do_not_even not_even_remember even_remember_slsp remember_slsp_happening slsp_happening_remember happening_remember_like remember_like_wtf like_wtf_lights wtf_lights_turned lights_turned_everyone turned_everyone_screamed everyone_screamed_encore</td>\n",
       "      <td>i dont even remember slsp happening i just remember being like wtf and then the lights turned off and everyone screamed for the encore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hazelannmac ooh feel guilty wishing hatman bet mudslide delicious hazelannmac_ooh ooh_feel feel_guilty guilty_wishing wishing_hatman hatman_bet bet_mudslide mudslide_delicious hazelannmac_ooh_feel ooh_feel_guilty feel_guilty_wishing guilty_wishing_hatman wishing_hatman_bet hatman_bet_mudslide bet_mudslide_delicious</td>\n",
       "      <td>@hazelannmac ooh now I feel guilty about wishing hatman out. I bet the mudslide was delicious!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     text_with_ngrams  \\\n",
       "0  do not even remember slsp happening remember like wtf lights turned everyone screamed encore do_not not_even even_remember remember_slsp slsp_happening happening_remember remember_like like_wtf wtf_lights lights_turned turned_everyone everyone_screamed screamed_encore do_not_even not_even_remember even_remember_slsp remember_slsp_happening slsp_happening_remember happening_remember_like remember_like_wtf like_wtf_lights wtf_lights_turned lights_turned_everyone turned_everyone_screamed everyone_screamed_encore   \n",
       "1                                                                                                                                                                                                        hazelannmac ooh feel guilty wishing hatman bet mudslide delicious hazelannmac_ooh ooh_feel feel_guilty guilty_wishing wishing_hatman hatman_bet bet_mudslide mudslide_delicious hazelannmac_ooh_feel ooh_feel_guilty feel_guilty_wishing guilty_wishing_hatman wishing_hatman_bet hatman_bet_mudslide bet_mudslide_delicious   \n",
       "\n",
       "                                                                                                                                 text_raw  \n",
       "0  i dont even remember slsp happening i just remember being like wtf and then the lights turned off and everyone screamed for the encore  \n",
       "1                                          @hazelannmac ooh now I feel guilty about wishing hatman out. I bet the mudslide was delicious!  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_ngrams(text, n):\n",
    "    tokens = word_tokenize(text)\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return ['_'.join(ngram) for ngram in n_grams]\n",
    "\n",
    "def add_ngrams_to_text(text):\n",
    "    bigrams_string = ' '.join(create_ngrams(text, 2))\n",
    "    trigrams_string = ' '.join(create_ngrams(text, 3))\n",
    "    return text + ' ' + bigrams_string + ' ' + trigrams_string\n",
    "\n",
    "def add_ngrams_to_df(df):\n",
    "    df['text_with_ngrams'] = df['text'].apply(add_ngrams_to_text)\n",
    "    return df\n",
    "\n",
    "# Apply to DataFrames\n",
    "df_train = add_ngrams_to_df(df_train)\n",
    "df_test = add_ngrams_to_df(df_test)\n",
    "\n",
    "# print full rows\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_train[['text_with_ngrams', 'text_raw']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if text_embedding['bert']:\n",
    "    df_train_text_embedded_bert = df_train.copy()\n",
    "    df_test_text_embedded_bert = df_test.copy()\n",
    "\n",
    "    # Load pre-trained BERT tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def get_bert_embeddings(text: str):\n",
    "        inputs = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state[:,0,:].detach().numpy()\n",
    "\n",
    "    # Generate BERT embeddings\n",
    "    df_train_text_embedded_bert['bert_embeddings'] = df_train_text_embedded_bert['text'].apply(get_bert_embeddings)\n",
    "    df_test_text_embedded_bert['bert_embeddings'] = df_test_text_embedded_bert['text'].apply(get_bert_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding `text` using `TF-IDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if text_embedding['tfidf']:\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "    feature_to_embed = 'text_with_ngrams'\n",
    "\n",
    "    # Fit and transform the training data\n",
    "    text_embedded = vectorizer.fit_transform(df_train[feature_to_embed])\n",
    "    df_train_text_embedded_tfidf = pd.DataFrame(text_embedded.toarray(), columns=vectorizer.get_feature_names_out(), index=df_train.index)\n",
    "\n",
    "    # Transform the test data using the same vectorizer\n",
    "    text_embedded_test = vectorizer.transform(df_test[feature_to_embed])\n",
    "    df_test_text_embedded_tfidf = pd.DataFrame(text_embedded_test.toarray(), columns=vectorizer.get_feature_names_out(), index=df_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding `text` column using `Word2Vec`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_config = {\n",
    "    \"vector_size\": 200,\n",
    "    \"with_ngrams\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "\n",
    "# if text_embedding['word2vec']:\n",
    "#     if w2v_config['with_ngrams']:\n",
    "#         tokenized_text = df_train['text_with_ngrams'].apply(lambda x: x.split())\n",
    "#     else:\n",
    "#         tokenized_text = df_train['text'].apply(lambda x: x.split())\n",
    "\n",
    "# import gensim.downloader as api\n",
    "# model_w2v = api.load(\"glove-twitter-200\")\n",
    "\n",
    "# model_w2v = gensim.models.Word2Vec(\n",
    "#             tokenized_text,\n",
    "#             vector_size=w2v_conifig['vector_size'], # desired no. of features/independent variables\n",
    "#             window=5, # context window size\n",
    "#             min_count=2, # Ignores all words with total frequency lower than 2.                                  \n",
    "#             sg = 1, # 1 for skip-gram model, 0 for CBOW\n",
    "#             negative = 10, # for negative sampling\n",
    "#             workers= 8, # no.of cores\n",
    "#             seed = 34\n",
    "# ) \n",
    "\n",
    "# https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314\n",
    "# Skip-gram: works well with a small amount of the training data, represents well even rare words or phrases.\n",
    "# CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words.\n",
    "\n",
    "# model_w2v.train(tokenized_text, total_examples= len(df_train), epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create embeddings from `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if text_embedding['word2vec']:\n",
    "    def embed_text_feature(df, col, model, vector_size):\n",
    "        def tokens_to_vectors(text_tokens) -> np.ndarray:\n",
    "            vectors = np.zeros((len(text_tokens), vector_size))\n",
    "\n",
    "            # embed each token (word-ish) in the text. If the token is not in the model's vocabulary, embed it as a zero vector.\n",
    "            for i, token in enumerate(text_tokens):\n",
    "                try:\n",
    "                    vectors[i] = model[token]\n",
    "                except KeyError:  # Token not in the model's vocabulary\n",
    "                    vectors[i] = np.zeros(vector_size)\n",
    "\n",
    "            # if all tokens were zero vectors, i.e. all words not in the model's vocabulary, return a zero vector\n",
    "            if np.all(vectors == 0):\n",
    "                return np.zeros(vector_size)\n",
    "            \n",
    "            return vectors.mean(axis=0)\n",
    "\n",
    "        embeddings = []\n",
    "        for tokens in df[col].apply(lambda x: x.split()):\n",
    "            embeddings.append(tokens_to_vectors(tokens))\n",
    "\n",
    "        return pd.DataFrame(np.vstack(embeddings), columns=[f'{col}_w2v_{i}' for i in range(vector_size)])\n",
    "\n",
    "    df_train_text_embedded_w2v = embed_text_feature(df_train, 'text', model_w2v, w2v_config['vector_size'])\n",
    "    df_test_text_embedded_w2v = embed_text_feature(df_test, 'text', model_w2v, w2v_config['vector_size'])\n",
    "\n",
    "    df_train_text_embedded_w2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['target', 'text', 'keyword', 'text_raw', 'text_length', 'hashtag_count',\n",
       "       'mention_count', 'has_url', 'text_with_ngrams'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5898, 1004)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_keep = ['target', 'text_length', 'hashtag_count', 'mention_count', 'has_url']\n",
    "\n",
    "X_train = pd.concat([\n",
    "    df_train[features_to_keep], \n",
    "    # df_train_text_embedded_w2v,\n",
    "    df_train_text_embedded_tfidf,\n",
    "    # df_train_text_embedded_bert\n",
    "    ], axis=1)\n",
    "X_test = pd.concat([\n",
    "    df_test[features_to_keep], \n",
    "    # df_test_text_embedded_w2v,\n",
    "    df_test_text_embedded_tfidf,\n",
    "    # df_test_text_embedded_bert\n",
    "    ], axis=1)\n",
    "\n",
    "X_train.dropna(inplace=True)\n",
    "\n",
    "# extract y_train and y_test here to avoid column name collision with 'target' feature coming from text and keyword embeddings\n",
    "y_train = X_train['target']\n",
    "y_test = X_test['target']\n",
    "\n",
    "X_train.drop(['target'], axis=1, inplace=True)\n",
    "X_test.drop(['target'], axis=1, inplace=True)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = True\n",
    "svm = False\n",
    "xgb = True\n",
    "random_forest = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(y_pred, y_train, y_pred_test, y_test):\n",
    "    print(\"Train results\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Train accuracy: {}\".format(accuracy_score(y_train, y_pred)))\n",
    "    print(classification_report(y_train, y_pred))\n",
    "    print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "    print()\n",
    "    print(\"Test results\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Test accuracy: {}\".format(accuracy_score(y_test, y_pred_test)))\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results\n",
      "-----------------------------\n",
      "Train accuracy: 0.8450322143099356\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88      3596\n",
      "           1       0.84      0.74      0.79      2302\n",
      "\n",
      "    accuracy                           0.85      5898\n",
      "   macro avg       0.84      0.83      0.83      5898\n",
      "weighted avg       0.84      0.85      0.84      5898\n",
      "\n",
      "[[3280  316]\n",
      " [ 598 1704]]\n",
      "\n",
      "Test results\n",
      "-----------------------------\n",
      "Test accuracy: 0.7881433823529411\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.82      1219\n",
      "           1       0.81      0.68      0.74       957\n",
      "\n",
      "    accuracy                           0.79      2176\n",
      "   macro avg       0.79      0.78      0.78      2176\n",
      "weighted avg       0.79      0.79      0.79      2176\n",
      "\n",
      "[[1068  151]\n",
      " [ 310  647]]\n"
     ]
    }
   ],
   "source": [
    "if logreg:\n",
    "    logreg = LogisticRegression(random_state=42, solver=\"liblinear\")\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = cross_val_predict(logreg, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "    y_pred_test = logreg.predict(X_test)\n",
    "\n",
    "    print_results(y_pred, y_train, y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text_raw</th>\n",
       "      <th>text_length</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>has_url</th>\n",
       "      <th>text_with_ngrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>car crash accident explosion fire</td>\n",
       "      <td>test</td>\n",
       "      <td>car crash accident explosion fire</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>car crash accident explosion fire car_crash crash_accident accident_explosion explosion_fire car_crash_accident crash_accident_explosion accident_explosion_fire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                text keyword  \\\n",
       "0  car crash accident explosion fire    test   \n",
       "\n",
       "                            text_raw  text_length  hashtag_count  \\\n",
       "0  car crash accident explosion fire           33              0   \n",
       "\n",
       "   mention_count  has_url  \\\n",
       "0              0        0   \n",
       "\n",
       "                                                                                                                                                   text_with_ngrams  \n",
       "0  car crash accident explosion fire car_crash crash_accident accident_explosion explosion_fire car_crash_accident crash_accident_explosion accident_explosion_fire  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1672569 0.8327431]]\n"
     ]
    }
   ],
   "source": [
    "# test manual prediction\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'text': ['car crash accident explosion fire'],\n",
    "    'keyword': 'test',\n",
    "})\n",
    "\n",
    "test_df = clean_data(test_df)\n",
    "test_df = extract_features(test_df)\n",
    "test_df = add_ngrams_to_df(test_df)\n",
    "\n",
    "feature_to_embed = 'text_with_ngrams'\n",
    "vectors = vectorizer.transform(test_df[feature_to_embed])\n",
    "text_embedded = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names_out(), index=test_df.index)\n",
    "\n",
    "display(test_df)\n",
    "\n",
    "# drop text_raw and text_with_ngrams\n",
    "test_df.drop(['text_raw', 'text_with_ngrams', 'keyword', 'text'], axis=1, inplace=True)\n",
    "\n",
    "test_df = pd.concat([\n",
    "    test_df,\n",
    "    text_embedded\n",
    "    ], axis=1)\n",
    "\n",
    "y_pred_test = logreg.predict_proba(test_df)\n",
    "\n",
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if svm:\n",
    "    first_n = 1000\n",
    "\n",
    "    # Initialize SVM model\n",
    "    svm_model = SVC(kernel='linear', C=1, random_state=42, probability=False)\n",
    "\n",
    "    # Fit the model on training data\n",
    "    svm_model.fit(X_train, y_train)\n",
    "\n",
    "    # Use 5-fold cross-validation to get predictions on training set\n",
    "    y_pred_train = cross_val_predict(svm_model, X_train, y_train, cv=5)\n",
    "    y_pred_test = svm_model.predict(X_test)\n",
    "\n",
    "    print_results(y_pred_train, y_train, y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Metrics:\n",
      "Accuracy: 0.8999660902000678\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92      3596\n",
      "           1       0.96      0.77      0.86      2302\n",
      "\n",
      "    accuracy                           0.90      5898\n",
      "   macro avg       0.92      0.88      0.89      5898\n",
      "weighted avg       0.91      0.90      0.90      5898\n",
      "\n",
      "\n",
      "Test Set Metrics:\n",
      "Accuracy: 0.7844669117647058\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.91      0.83      1219\n",
      "           1       0.84      0.63      0.72       957\n",
      "\n",
      "    accuracy                           0.78      2176\n",
      "   macro avg       0.80      0.77      0.77      2176\n",
      "weighted avg       0.79      0.78      0.78      2176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Creating an XGBoost classifier\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# Training the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "predictions_test = model.predict(X_test)\n",
    "\n",
    "# Calculating accuracy on test set\n",
    "accuracy_test = accuracy_score(y_test, predictions_test)\n",
    "\n",
    "# Making predictions on the training set\n",
    "predictions_train = model.predict(X_train)\n",
    "\n",
    "# Calculating accuracy on training set\n",
    "accuracy_train = accuracy_score(y_train, predictions_train)\n",
    "\n",
    "print(\"\\nTraining Set Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_train)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_train, predictions_train))\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if random_forest:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results\n",
      "-----------------------------\n",
      "Train accuracy: 0.7848423194303153\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.93      0.84      3596\n",
      "           1       0.84      0.56      0.67      2302\n",
      "\n",
      "    accuracy                           0.78      5898\n",
      "   macro avg       0.80      0.74      0.75      5898\n",
      "weighted avg       0.79      0.78      0.77      5898\n",
      "\n",
      "[[3348  248]\n",
      " [1021 1281]]\n",
      "\n",
      "Test results\n",
      "-----------------------------\n",
      "Test accuracy: 0.7293198529411765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.90      0.79      1219\n",
      "           1       0.80      0.52      0.63       957\n",
      "\n",
      "    accuracy                           0.73      2176\n",
      "   macro avg       0.75      0.71      0.71      2176\n",
      "weighted avg       0.74      0.73      0.72      2176\n",
      "\n",
      "[[1093  126]\n",
      " [ 463  494]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Initialize AdaBoost Classifier\n",
    "adaboost = AdaBoostClassifier(random_state=0)\n",
    "\n",
    "# Perform 5-fold cross-validation on the training data\n",
    "y_pred_cv = cross_val_predict(adaboost, X_train, y_train, cv=5)\n",
    "\n",
    "# Fit the model on the entire training data\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_test = adaboost.predict(X_test)\n",
    "\n",
    "# Evaluate and print the results\n",
    "print_results(y_pred_cv, y_train, y_pred_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant tweets: 2302\n",
      "Not relevant tweets: 3596\n"
     ]
    }
   ],
   "source": [
    "# Check for data imbalance. Check how many tweets are relevant and how many are not relevant.\n",
    "relevant_tweets = df_train[df_train['target'] == 1]\n",
    "not_relevant_tweets = df_train[df_train['target'] == 0]\n",
    "\n",
    "print(\"Relevant tweets: {}\".format(relevant_tweets.shape[0]))\n",
    "print(\"Not relevant tweets: {}\".format(not_relevant_tweets.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=200; total time=  24.6s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=200; total time=  24.7s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=200; total time=  24.8s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=200; total time=  25.0s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=200; total time=  25.1s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=250; total time=  30.8s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=250; total time=  31.1s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=250; total time=  31.1s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=250; total time=  31.1s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=250; total time=  31.1s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=300; total time=  38.1s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=300; total time=  38.4s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=200; total time=  26.8s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=200; total time=  27.1s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=200; total time=  27.2s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=200; total time=  27.0s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=200; total time=  29.2s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=300; total time=  41.0s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=250; total time=  34.7s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=300; total time=  41.3s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=250; total time=  35.1s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=300; total time=  45.6s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=250; total time=  35.4s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=250; total time=  38.0s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=250; total time=  36.4s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=200; total time=  25.2s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=200; total time=  25.2s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=200; total time=  25.1s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=300; total time=  39.5s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=200; total time=  25.8s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=300; total time=  38.5s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=300; total time=  38.4s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=200; total time=  25.1s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=300; total time=  41.3s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=300; total time=  36.2s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=250; total time=  28.5s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=250; total time=  27.8s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=250; total time=  27.8s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=250; total time=  27.8s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=250; total time=  30.7s\n",
      "[CV] END .algorithm=SAMME, learning_rate=2, n_estimators=200; total time=  23.3s\n",
      "[CV] END .algorithm=SAMME, learning_rate=2, n_estimators=200; total time=  23.4s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=300; total time=  34.7s\n",
      "[CV] END .algorithm=SAMME, learning_rate=2, n_estimators=200; total time=  23.7s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=300; total time=  35.0s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=300; total time=  34.9s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=300; total time=  35.1s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=300; total time=  37.8s\n",
      "[CV] END .algorithm=SAMME, learning_rate=2, n_estimators=200; total time=  26.0s\n",
      "[CV] END .algorithm=SAMME, learning_rate=2, n_estimators=200; total time=  23.8s\n",
      "[CV] END .algorithm=SAMME, learning_rate=2, n_estimators=250; total time=  29.6s\n",
      "[CV] END .algorithm=SAMME, learning_rate=2, n_estimators=250; total time=  29.4s\n",
      "[CV] END .algorithm=SAMME, learning_rate=2, n_estimators=250; total time=  28.5s\n",
      "[CV] END .algorithm=SAMME, learning_rate=2, n_estimators=250; total time=  28.8s\n",
      "[CV] END .algorithm=SAMME, learning_rate=2, n_estimators=250; total time=  31.7s\n",
      "[CV] END .algorithm=SAMME, learning_rate=2, n_estimators=300; total time=  34.2s\n",
      "[CV] END .algorithm=SAMME, learning_rate=3, n_estimators=200; total time=  23.0s\n",
      "[CV] END .algorithm=SAMME, learning_rate=2, n_estimators=300; total time=  34.2s\n",
      "[CV] END .algorithm=SAMME, learning_rate=3, n_estimators=200; total time=  22.9s\n",
      "[CV] END .algorithm=SAMME, learning_rate=2, n_estimators=300; total time=  34.1s\n",
      "[CV] END .algorithm=SAMME, learning_rate=2, n_estimators=300; total time=  37.7s\n",
      "[CV] END .algorithm=SAMME, learning_rate=3, n_estimators=200; total time=  23.0s\n",
      "[CV] END .algorithm=SAMME, learning_rate=2, n_estimators=300; total time=  37.5s\n",
      "[CV] END .algorithm=SAMME, learning_rate=3, n_estimators=200; total time=  25.2s\n",
      "[CV] END .algorithm=SAMME, learning_rate=3, n_estimators=200; total time=  23.0s\n",
      "[CV] END .algorithm=SAMME, learning_rate=3, n_estimators=250; total time=  28.5s\n",
      "[CV] END .algorithm=SAMME, learning_rate=3, n_estimators=250; total time=  28.6s\n",
      "[CV] END .algorithm=SAMME, learning_rate=3, n_estimators=250; total time=  28.3s\n",
      "[CV] END .algorithm=SAMME, learning_rate=3, n_estimators=250; total time=  30.6s\n",
      "[CV] END .algorithm=SAMME, learning_rate=3, n_estimators=250; total time=  30.8s\n",
      "[CV] END .algorithm=SAMME, learning_rate=3, n_estimators=300; total time=  33.5s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=200; total time=  22.7s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=200; total time=  22.5s\n",
      "[CV] END .algorithm=SAMME, learning_rate=3, n_estimators=300; total time=  33.8s\n",
      "[CV] END .algorithm=SAMME, learning_rate=3, n_estimators=300; total time=  33.6s\n",
      "[CV] END .algorithm=SAMME, learning_rate=3, n_estimators=300; total time=  33.5s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=200; total time=  22.8s\n",
      "[CV] END .algorithm=SAMME, learning_rate=3, n_estimators=300; total time=  36.7s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=200; total time=  24.7s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=200; total time=  24.7s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=250; total time=  27.8s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=250; total time=  28.0s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=250; total time=  27.5s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=250; total time=  27.7s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=250; total time=  30.3s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=300; total time=  33.0s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=200; total time=  22.1s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=300; total time=  33.1s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=300; total time=  33.2s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=200; total time=  22.2s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=300; total time=  36.4s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=200; total time=  22.7s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=300; total time=  36.5s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=200; total time=  24.8s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=200; total time=  22.7s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=250; total time=  28.6s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=250; total time=  28.5s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=250; total time=  28.5s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=250; total time=  31.0s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=250; total time=  31.0s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=200; total time=  22.7s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=300; total time=  34.2s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=200; total time=  22.8s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=300; total time=  34.3s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=300; total time=  34.0s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=200; total time=  22.6s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=200; total time=  24.7s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=200; total time=  22.5s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=300; total time=  37.6s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=300; total time=  37.4s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=250; total time=  28.5s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=250; total time=  28.4s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=250; total time=  28.3s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=250; total time=  28.2s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=250; total time=  28.2s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=2, n_estimators=200; total time=  22.6s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=300; total time=  34.0s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=2, n_estimators=200; total time=  22.6s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=300; total time=  34.1s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=300; total time=  33.9s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=2, n_estimators=200; total time=  22.8s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=300; total time=  33.6s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=300; total time=  36.9s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=2, n_estimators=200; total time=  24.7s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=2, n_estimators=200; total time=  24.7s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=2, n_estimators=250; total time=  28.0s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=2, n_estimators=250; total time=  28.2s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=2, n_estimators=250; total time=  27.9s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=2, n_estimators=250; total time=  30.5s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=2, n_estimators=250; total time=  30.2s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=3, n_estimators=200; total time=  22.6s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=2, n_estimators=300; total time=  33.7s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=2, n_estimators=300; total time=  33.8s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=3, n_estimators=200; total time=  22.8s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=2, n_estimators=300; total time=  34.5s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=2, n_estimators=300; total time=  34.0s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=3, n_estimators=200; total time=  22.9s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=3, n_estimators=200; total time=  23.0s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=2, n_estimators=300; total time=  37.5s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=3, n_estimators=200; total time=  25.7s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=3, n_estimators=250; total time=  28.1s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=3, n_estimators=250; total time=  27.8s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=3, n_estimators=250; total time=  26.1s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=3, n_estimators=250; total time=  24.8s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=3, n_estimators=250; total time=  27.7s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=3, n_estimators=300; total time=  26.6s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=3, n_estimators=300; total time=  25.3s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=3, n_estimators=300; total time=  24.8s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=3, n_estimators=300; total time=  24.3s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=3, n_estimators=300; total time=  24.4s\n",
      "Best Parameters: {'algorithm': 'SAMME.R', 'learning_rate': 1, 'n_estimators': 250}\n",
      "Train results\n",
      "-----------------------------\n",
      "Train accuracy: 0.8294337063411326\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86      3596\n",
      "           1       0.81      0.74      0.77      2302\n",
      "\n",
      "    accuracy                           0.83      5898\n",
      "   macro avg       0.83      0.81      0.82      5898\n",
      "weighted avg       0.83      0.83      0.83      5898\n",
      "\n",
      "[[3200  396]\n",
      " [ 610 1692]]\n",
      "\n",
      "Test results\n",
      "-----------------------------\n",
      "Test accuracy: 0.7876838235294118\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82      1219\n",
      "           1       0.81      0.68      0.74       957\n",
      "\n",
      "    accuracy                           0.79      2176\n",
      "   macro avg       0.79      0.78      0.78      2176\n",
      "weighted avg       0.79      0.79      0.78      2176\n",
      "\n",
      "[[1067  152]\n",
      " [ 310  647]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if hyperparameter_tuning['adaboost']:\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [200, 250, 300],\n",
    "        'learning_rate': [0.01, 0.1, 1, 2, 3],\n",
    "        'algorithm': ['SAMME', 'SAMME.R']\n",
    "    }\n",
    "\n",
    "    # Create GridSearchCV object\n",
    "    grid_search = GridSearchCV(estimator=adaboost, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print best parameters\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Generate cross-validated predictions for the training set\n",
    "    y_pred_train = cross_val_predict(best_model, X_train, y_train, cv=5)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "    print_results(y_pred_train, y_train, y_pred_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
