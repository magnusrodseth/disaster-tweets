{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IT3212 - Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eda = False\n",
    "lemmatize = False\n",
    "with_sentiment = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from tensorflow.keras.preprocessing import sequence, text\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix dataset encoding issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some rows in the raw data include non UTF-8 characters. \n",
    "\n",
    "# Example of text with non UTF-8 characters:\n",
    "# 778245336,FALSE,finalized,5,8/30/15 13:27,Not Relevant,0.7952,,army,\n",
    "# text column: Pakistan,\".: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: RT DrAyesha4: #IndiaKoMunTorJawabDo Indian Army ki��_ http://t.co/WJLJq3yA4g\"\n",
    "# ,6.29079E+17,195397186\n",
    "\n",
    "# Chardet identifies the encoding of the raw data as 'MacRoman'.\n",
    "# For now, we will remove all non UTF-8 characters from the raw data\n",
    "# We handle this by removing all � characters from the raw data and writing the modified content back to the file.\n",
    "\n",
    "def fix_non_utf8_encoding(filepath, destination_filepath):\n",
    "    with open(filepath, 'rb') as file:\n",
    "        rawdata = file.read()\n",
    "        result = chardet.detect(rawdata)\n",
    "        print(result['encoding'])\n",
    "\n",
    "\n",
    "    # Open the file in read mode, read its contents, then close it\n",
    "    with open('data/disaster-tweets.csv', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Remove all � characters\n",
    "    content = content.replace('�', '')\n",
    "\n",
    "    # Open the file in write mode and write the modified content back to it\n",
    "    with open(destination_filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "filepath = 'data/disaster-tweets.csv'\n",
    "dest = 'data/disaster-tweets-utf8.csv'\n",
    "\n",
    "# fix_non_utf8_encoding(filepath, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(filepath, destination_filepath_train, destination_filepath_test):\n",
    "    df = pd.read_csv(filepath, encoding='utf-8')\n",
    "    train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "def split_train_val_test(filepath):\n",
    "    df = pd.read_csv(filepath, encoding='utf-8')\n",
    "    train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    val_data = val_data.reset_index(drop=True)\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test = split_train_val_test(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['choose_one', 'keyword', 'location', 'text'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.drop([\n",
    "    '_unit_id',\n",
    "    '_golden',\n",
    "    '_unit_state',\n",
    "    '_trusted_judgments',\n",
    "    '_last_judgment_at',\n",
    "    'choose_one:confidence',\n",
    "    'choose_one_gold',\n",
    "    'tweetid',\n",
    "    'userid'\n",
    "    # 'choose_one',\n",
    "    # 'text',\n",
    "    ], axis=1, inplace=True)\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6960, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'].apply(lambda x:len(str(x).split())).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordNetLemmatizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/skog/semester/subjects/IT3212-data-powered-software/repo/notebook-new_advanced-2.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/skog/semester/subjects/IT3212-data-powered-software/repo/notebook-new_advanced-2.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/skog/semester/subjects/IT3212-data-powered-software/repo/notebook-new_advanced-2.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m TweetTokenizer()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/skog/semester/subjects/IT3212-data-powered-software/repo/notebook-new_advanced-2.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfilter_rows_by_confidence_and_decision\u001b[39m(df, confidence_threshold):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordNetLemmatizer' is not defined"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def filter_rows_by_confidence_and_decision(df, confidence_threshold):\n",
    "    df = df[df['choose_one:confidence'] >= confidence_threshold]\n",
    "    df = df[df['choose_one'] != \"Can't Decide\"]\n",
    "    return df\n",
    "\n",
    "def map_choose_one_to_y(df):\n",
    "    df['target'] = df['choose_one'].apply(lambda choice: 1 if choice == 'Relevant' else 0)\n",
    "    return df\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words(\"english\")])\n",
    "    if lemmatize:\n",
    "        text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    text = contractions.fix(text)\n",
    "    text = ' '.join(tokenizer.tokenize(text))\n",
    "    return text\n",
    "\n",
    "def clean_keyword(keyword):\n",
    "    return unquote(keyword) if pd.notnull(keyword) else ''\n",
    "\n",
    "def clean_data(df):\n",
    "    df['keyword'] = df['keyword'].apply(clean_keyword).apply(str.lower)\n",
    "    df['text_raw'] = df['text']\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "initial_count = df_train.shape[0]\n",
    "confidence_threshold = 0.7\n",
    "\n",
    "df_train = filter_rows_by_confidence_and_decision(df_train, confidence_threshold)\n",
    "print(\"Removed {} of total: {} rows. Remaining rows: {}\".format(initial_count - df_train.shape[0], initial_count, df_train.shape[0]))\n",
    "\n",
    "features_to_keep = ['target', 'text', 'keyword']\n",
    "\n",
    "df_train = map_choose_one_to_y(df_train)\n",
    "df_train = df_train[features_to_keep]\n",
    "df_train = clean_data(df_train)\n",
    "\n",
    "count_initial = df_train.shape[0]\n",
    "df_train = df_train.drop_duplicates(subset=['text'])\n",
    "print(\"Removed {} duplicated rows.\".format(count_initial - df_train.shape[0]))\n",
    "\n",
    "\n",
    "# Preprocess the test data as well\n",
    "df_test = map_choose_one_to_y(df_test)\n",
    "df_test = df_test[features_to_keep]\n",
    "df_test = clean_data(df_test)\n",
    "\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features that can be extracted from the raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text_raw</th>\n",
       "      <th>text_length</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>has_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>do not even remember slsp happening remember l...</td>\n",
       "      <td>screamed</td>\n",
       "      <td>i dont even remember slsp happening i just rem...</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>hazelannmac ooh feel guilty wishing hatman bet...</td>\n",
       "      <td>mudslide</td>\n",
       "      <td>@hazelannmac ooh now I feel guilty about wishi...</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>soultech collide club mix</td>\n",
       "      <td>collide</td>\n",
       "      <td>Soultech - Collide (Club Mix) http://t.co/8xIx...</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>police officer wounded suspect dead exchanging...</td>\n",
       "      <td>wounded</td>\n",
       "      <td>Police Officer Wounded Suspect Dead After Exch...</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>cramer igers 3 words wrecked disneys stock</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Cramer: Iger's 3 words that wrecked Disney's s...</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text   keyword  \\\n",
       "0       0  do not even remember slsp happening remember l...  screamed   \n",
       "1       0  hazelannmac ooh feel guilty wishing hatman bet...  mudslide   \n",
       "2       0                          soultech collide club mix   collide   \n",
       "3       1  police officer wounded suspect dead exchanging...   wounded   \n",
       "4       0         cramer igers 3 words wrecked disneys stock   wrecked   \n",
       "\n",
       "                                            text_raw  text_length  \\\n",
       "0  i dont even remember slsp happening i just rem...          134   \n",
       "1  @hazelannmac ooh now I feel guilty about wishi...           94   \n",
       "2  Soultech - Collide (Club Mix) http://t.co/8xIx...           52   \n",
       "3  Police Officer Wounded Suspect Dead After Exch...           83   \n",
       "4  Cramer: Iger's 3 words that wrecked Disney's s...           73   \n",
       "\n",
       "   hashtag_count  mention_count  has_url  \n",
       "0              0              0        0  \n",
       "1              0              1        0  \n",
       "2              0              0        1  \n",
       "3              0              0        1  \n",
       "4              0              0        1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features(df): \n",
    "    # Create new column for text length\n",
    "    df['text_length'] = df['text_raw'].apply(len)\n",
    "    # Extract the number of hashtags\n",
    "    df[\"hashtag_count\"] = df[\"text_raw\"].apply(lambda x: len([c for c in str(x) if c == \"#\"]))\n",
    "\n",
    "    # Extract the number of mentions\n",
    "    df[\"mention_count\"] = df[\"text_raw\"].apply(lambda x: len([c for c in str(x) if c == \"@\"]))\n",
    "\n",
    "    # Extract the `has_url` feature\n",
    "    df[\"has_url\"] = df[\"text_raw\"].apply(lambda x: 1 if \"http\" in str(x) else 0)\n",
    "    return df\n",
    "\n",
    "# Write the updated dataframe to a CSV file\n",
    "df_train = extract_features(df_train)\n",
    "df_test = extract_features(df_test)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(text, n):\n",
    "    tokens = word_tokenize(text)\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return n_grams\n",
    "\n",
    "\n",
    "df_train['bigrams'] = df_train['text'].apply(lambda x: create_ngrams(x, 2))\n",
    "df_train['trigrams'] = df_train['text'].apply(lambda x: create_ngrams(x, 3))\n",
    "\n",
    "df_test['bigrams'] = df_test['text'].apply(lambda x: create_ngrams(x, 2))\n",
    "df_test['trigrams'] = df_test['text'].apply(lambda x: create_ngrams(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams_string(ngram_list):\n",
    "    ngram_words = ['_'.join(ngram) for ngram in ngram_list]\n",
    "    ngram_string = ' '.join(ngram_words)\n",
    "    return ngram_string\n",
    "\n",
    "def add_ngrams(df):\n",
    "    df['bigrams'] = df['bigrams'].apply(lambda x: create_ngrams_string(x))\n",
    "    df['trigrams'] = df['trigrams'].apply(lambda x: create_ngrams_string(x))\n",
    "\n",
    "    df['text_with_ngrams'] = df['text'] + ' ' +  df['bigrams'] + ' ' + df['trigrams'] \n",
    "    return df\n",
    "\n",
    "df_train = add_ngrams(df_train)\n",
    "df_test = add_ngrams(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6569397, 15305800)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = df_train['text_with_ngrams'].apply(lambda x: x.split())\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_text,\n",
    "            vector_size=400, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2, # Ignores all words with total frequency lower than 2.                                  \n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 32, # no.of cores\n",
    "            seed = 34\n",
    ") \n",
    "\n",
    "model_w2v.train(tokenized_text, total_examples= len(df_train), epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gz/34s3tc0d1qz3drk0v83_097h0000gn/T/ipykernel_54315/1470173752.py:9: RuntimeWarning: Mean of empty slice.\n",
      "  return vectors.mean(axis=0)\n",
      "/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "# Define a function that converts tokens to vectors using the Word2Vec model\n",
    "def tokens_to_vectors(tokens, model, vector_size) -> np.ndarray:\n",
    "    vectors = np.zeros((len(tokens), vector_size))\n",
    "    for i, token in enumerate(tokens):\n",
    "        try:\n",
    "            vectors[i] = model.wv[token]\n",
    "        except KeyError:  # Token not in the model's vocabulary\n",
    "            vectors[i] = np.zeros(vector_size)\n",
    "    return vectors.mean(axis=0)\n",
    "\n",
    "# Example usage\n",
    "vecs_train = [tokens_to_vectors(tokens, model_w2v, 400) for tokens in tokenized_text]\n",
    "vecs_train = np.vstack(vecs_train)\n",
    "\n",
    "vecs_test = [tokens_to_vectors(tokens, model_w2v, 400) for tokens in df_test['text'].apply(lambda x: x.split())]\n",
    "vecs_test = np.vstack(vecs_test)\n",
    "# # Converting the list of vectors to a DataFrame\n",
    "# vectors_df = pd.DataFrame(np.vstack(vecs))\n",
    "# print(vectors_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedded_w2v_df = pd.DataFrame(vecs_train, columns=[str(x) for x in range(400)], index=df_train.index) \n",
    "text_embedded_w2f_df_test = pd.DataFrame(vecs_test, columns=[str(x) for x in range(400)], index=df_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing this for later we can test different features without having to re-run cells above this one\n",
    "df_checkpoint = df_train.copy(deep=True)\n",
    "df_test_checkpoint = df_test.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['target', 'text', 'keyword', 'text_raw', 'text_length', 'hashtag_count',\n",
       "       'mention_count', 'has_url', 'bigrams', 'trigrams', 'text_with_ngrams'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = ['target', 'text_length', 'hashtag_count', 'mention_count', 'has_url']\n",
    "\n",
    "df_checkpoint = df_train[features_to_keep]\n",
    "df_test_checkpoint = df_test[features_to_keep]\n",
    "\n",
    "\n",
    "df_checkpoint = pd.concat([df_checkpoint, text_embedded_w2v_df], axis=1)\n",
    "df_test_checkpoint = pd.concat([df_test_checkpoint, text_embedded_w2f_df_test], axis=1)\n",
    "\n",
    "df_checkpoint.dropna(inplace=True)\n",
    "df_test_checkpoint.dropna(inplace=True)\n",
    "\n",
    "# extract y_train and y_test here to avoid column name collision with 'target' feature coming from text and keyword embeddings\n",
    "y_train = df_checkpoint['target']\n",
    "y_test = df_test_checkpoint['target']\n",
    "\n",
    "X_train = df_checkpoint.drop(['target'], axis=1)\n",
    "X_test = df_test_checkpoint.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = True\n",
    "svm = False\n",
    "xgb = False\n",
    "rforest = False\n",
    "lightgbm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(y_pred, y_train, y_pred_test, y_test):\n",
    "    print(\"Train results\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Train accuracy: {}\".format(accuracy_score(y_train, y_pred)))\n",
    "    print(classification_report(y_train, y_pred))\n",
    "    print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "    print()\n",
    "    print(\"Test results\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Test accuracy: {}\".format(accuracy_score(y_test, y_pred_test)))\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results\n",
      "-----------------------------\n",
      "Train accuracy: 0.8475496014922842\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88      3595\n",
      "           1       0.85      0.74      0.79      2302\n",
      "\n",
      "    accuracy                           0.85      5897\n",
      "   macro avg       0.85      0.83      0.84      5897\n",
      "weighted avg       0.85      0.85      0.85      5897\n",
      "\n",
      "[[3305  290]\n",
      " [ 609 1693]]\n",
      "\n",
      "Test results\n",
      "-----------------------------\n",
      "Test accuracy: 0.7665441176470589\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78      1219\n",
      "           1       0.71      0.79      0.75       957\n",
      "\n",
      "    accuracy                           0.77      2176\n",
      "   macro avg       0.77      0.77      0.77      2176\n",
      "weighted avg       0.77      0.77      0.77      2176\n",
      "\n",
      "[[912 307]\n",
      " [201 756]]\n"
     ]
    }
   ],
   "source": [
    "if logreg:\n",
    "    logreg = LogisticRegression(random_state=42, solver=\"liblinear\")\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = cross_val_predict(logreg, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "    y_pred_test = logreg.predict(X_test)\n",
    "\n",
    "    print_results(y_pred, y_train, y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results\n",
      "-----------------------------\n",
      "Train accuracy: 0.8494149567576734\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88      3595\n",
      "           1       0.86      0.74      0.79      2302\n",
      "\n",
      "    accuracy                           0.85      5897\n",
      "   macro avg       0.85      0.83      0.84      5897\n",
      "weighted avg       0.85      0.85      0.85      5897\n",
      "\n",
      "[[3315  280]\n",
      " [ 608 1694]]\n",
      "\n",
      "Test results\n",
      "-----------------------------\n",
      "Test accuracy: 0.7614889705882353\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.74      0.78      1219\n",
      "           1       0.71      0.79      0.74       957\n",
      "\n",
      "    accuracy                           0.76      2176\n",
      "   macro avg       0.76      0.76      0.76      2176\n",
      "weighted avg       0.77      0.76      0.76      2176\n",
      "\n",
      "[[904 315]\n",
      " [204 753]]\n"
     ]
    }
   ],
   "source": [
    "if svm:\n",
    "    first_n = 1000\n",
    "\n",
    "    # Initialize SVM model\n",
    "    svm_model = SVC(kernel='linear', C=1, random_state=42, probability=False)\n",
    "\n",
    "    # Fit the model on training data\n",
    "    svm_model.fit(X_train, y_train)\n",
    "\n",
    "    # Use 5-fold cross-validation to get predictions on training set\n",
    "    y_pred_train = cross_val_predict(svm_model, X_train, y_train, cv=5)\n",
    "    y_pred_test = svm_model.predict(X_test)\n",
    "\n",
    "    print_results(y_pred_train, y_train, y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/skog/semester/subjects/IT3212-data-powered-software/repo/notebook-new_advanced-2.ipynb Cell 38\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/skog/semester/subjects/IT3212-data-powered-software/repo/notebook-new_advanced-2.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m xgb:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/skog/semester/subjects/IT3212-data-powered-software/repo/notebook-new_advanced-2.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mxgboost\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mxgb\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/skog/semester/subjects/IT3212-data-powered-software/repo/notebook-new_advanced-2.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score, classification_report\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb' is not defined"
     ]
    }
   ],
   "source": [
    "if xgb:\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "    params_edited = {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 7, 'min_child_weight': 1, 'subsample': 0.9}\n",
    "\n",
    "    #Creating an XGBoost classifier\n",
    "    model = xgb.XGBClassifier(params_edited)\n",
    "\n",
    "    #Training the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    #Making predictions on the test set\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    #Calculating accuracy\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.734375\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.78      0.77      1219\n",
      "           1       0.71      0.68      0.69       957\n",
      "\n",
      "    accuracy                           0.73      2176\n",
      "   macro avg       0.73      0.73      0.73      2176\n",
      "weighted avg       0.73      0.73      0.73      2176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if rforest:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5803965\ttotal: 60ms\tremaining: 60ms\n",
      "1:\tlearn: 0.5374717\ttotal: 65.4ms\tremaining: 0us\n",
      "Accuracy: 0.6677389705882353\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.69      0.70      1219\n",
      "           1       0.62      0.63      0.63       957\n",
      "\n",
      "    accuracy                           0.67      2176\n",
      "   macro avg       0.66      0.66      0.66      2176\n",
      "weighted avg       0.67      0.67      0.67      2176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# catboost\n",
    "\n",
    "import catboost as cb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# Initialize CatBoostClassifier\n",
    "\n",
    "model = CatBoostClassifier(iterations=2,\n",
    "                            learning_rate=1,\n",
    "                            depth=2,\n",
    "                            loss_function='Logloss',\n",
    "                            verbose=True)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted classes\n",
    "preds_class = model.predict(X_test)\n",
    "\n",
    "# print results\n",
    "print(\"Accuracy:\", accuracy_score(y_test, preds_class))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, preds_class))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas.core.strings' has no attribute 'StringMethods'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/skog/semester/subjects/IT3212-data-powered-software/repo/notebook-new_advanced.ipynb Cell 40\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/skog/semester/subjects/IT3212-data-powered-software/repo/notebook-new_advanced.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# lightgbm\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/skog/semester/subjects/IT3212-data-powered-software/repo/notebook-new_advanced.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlightgbm\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mlgb\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/skog/semester/subjects/IT3212-data-powered-software/repo/notebook-new_advanced.ipynb#X54sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/skog/semester/subjects/IT3212-data-powered-software/repo/notebook-new_advanced.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#Creating an XGBoost classifier\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/lightgbm/__init__.py:8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m\"\"\"LightGBM, Light Gradient Boosting Machine.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[39mContributors: https://github.com/microsoft/LightGBM/graphs/contributors.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbasic\u001b[39;00m \u001b[39mimport\u001b[39;00m Booster, Dataset, Sequence, register_logger\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcallback\u001b[39;00m \u001b[39mimport\u001b[39;00m early_stopping, log_evaluation, print_evaluation, record_evaluation, reset_parameter\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m CVBooster, cv, train\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/lightgbm/basic.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m PANDAS_INSTALLED, concat, dt_DataTable, is_dtype_sparse, pd_DataFrame, pd_Series\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlibpath\u001b[39;00m \u001b[39mimport\u001b[39;00m find_lib_path\n\u001b[1;32m     23\u001b[0m ZERO_THRESHOLD \u001b[39m=\u001b[39m \u001b[39m1e-35\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/lightgbm/compat.py:130\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marray\u001b[39;00m \u001b[39mimport\u001b[39;00m from_delayed \u001b[39mas\u001b[39;00m dask_array_from_delayed\n\u001b[1;32m    129\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbag\u001b[39;00m \u001b[39mimport\u001b[39;00m from_delayed \u001b[39mas\u001b[39;00m dask_bag_from_delayed\n\u001b[0;32m--> 130\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataframe\u001b[39;00m \u001b[39mimport\u001b[39;00m DataFrame \u001b[39mas\u001b[39;00m dask_DataFrame\n\u001b[1;32m    131\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataframe\u001b[39;00m \u001b[39mimport\u001b[39;00m Series \u001b[39mas\u001b[39;00m dask_Series\n\u001b[1;32m    132\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m \u001b[39mimport\u001b[39;00m Client, default_client, wait\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/dask/dataframe/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m compute\n\u001b[0;32m----> 3\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m backends, dispatch, rolling\n\u001b[1;32m      4\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m         DataFrame,\n\u001b[1;32m      6\u001b[0m         Index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m         to_timedelta,\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgroupby\u001b[39;00m \u001b[39mimport\u001b[39;00m Aggregation\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/dask/dataframe/backends.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msizeof\u001b[39;00m \u001b[39mimport\u001b[39;00m SimpleSizeof, sizeof\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m is_arraylike, typename\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m DataFrame, Index, Scalar, Series, _Frame\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdispatch\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     categorical_dtype_dispatch,\n\u001b[1;32m     23\u001b[0m     concat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     union_categoricals_dispatch,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mextensions\u001b[39;00m \u001b[39mimport\u001b[39;00m make_array_nonempty, make_scalar\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/dask/dataframe/core.py:52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwidgets\u001b[39;00m \u001b[39mimport\u001b[39;00m get_template\n\u001b[1;32m     51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m methods\n\u001b[0;32m---> 52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39maccessor\u001b[39;00m \u001b[39mimport\u001b[39;00m DatetimeAccessor, StringAccessor\n\u001b[1;32m     53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcategorical\u001b[39;00m \u001b[39mimport\u001b[39;00m CategoricalAccessor, categorize\n\u001b[1;32m     54\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdispatch\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     55\u001b[0m     get_parallel_type,\n\u001b[1;32m     56\u001b[0m     group_split_dispatch,\n\u001b[1;32m     57\u001b[0m     hash_object_dispatch,\n\u001b[1;32m     58\u001b[0m     meta_nonempty,\n\u001b[1;32m     59\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/dask/dataframe/accessor.py:109\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Accessor object for datetimelike properties of the Series values.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[1;32m    100\u001b[0m \u001b[39m    Examples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m    >>> s.dt.microsecond  # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     _accessor_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 109\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mStringAccessor\u001b[39;00m(Accessor):\n\u001b[1;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Accessor object for string properties of the Series values.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m \u001b[39m    Examples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m    >>> s.str.lower()  # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     _accessor_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstr\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ag/lib/python3.10/site-packages/dask/dataframe/accessor.py:121\u001b[0m, in \u001b[0;36mStringAccessor\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m _accessor_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstr\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m _not_implemented \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mget_dummies\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m--> 121\u001b[0m \u001b[39m@derived_from\u001b[39m(pd\u001b[39m.\u001b[39;49mcore\u001b[39m.\u001b[39;49mstrings\u001b[39m.\u001b[39;49mStringMethods)\n\u001b[1;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit\u001b[39m(\u001b[39mself\u001b[39m, pat\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, n\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, expand\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    123\u001b[0m     \u001b[39mif\u001b[39;00m expand:\n\u001b[1;32m    124\u001b[0m         \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas.core.strings' has no attribute 'StringMethods'"
     ]
    }
   ],
   "source": [
    "# lightgbm\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#Creating an XGBoost classifier\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "#Training the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Making predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "#Calculating accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
