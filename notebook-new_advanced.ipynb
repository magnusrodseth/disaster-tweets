{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IT3212 - Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eda = False\n",
    "lemmatize = False\n",
    "with_sentiment = False\n",
    "\n",
    "text_embedding = {\n",
    "    'tfidf': True,\n",
    "    'word2vec': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# NLTK tools and datasets\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import gensim\n",
    "\n",
    "# Uncomment if you need to download NLTK data packages\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "# Text processing\n",
    "from textblob import TextBlob\n",
    "import contractions\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, classification_report, \n",
    "                             confusion_matrix, roc_curve, auc)\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Miscellaneous\n",
    "from collections import Counter\n",
    "from urllib.parse import unquote\n",
    "from scipy import stats\n",
    "import chardet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix dataset encoding issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some rows in the raw data include non UTF-8 characters. \n",
    "\n",
    "# Example of text with non UTF-8 characters:\n",
    "# 778245336,FALSE,finalized,5,8/30/15 13:27,Not Relevant,0.7952,,army,\n",
    "# text column: Pakistan,\".: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: RT DrAyesha4: #IndiaKoMunTorJawabDo Indian Army ki��_ http://t.co/WJLJq3yA4g\"\n",
    "# ,6.29079E+17,195397186\n",
    "\n",
    "# Chardet identifies the encoding of the raw data as 'MacRoman'.\n",
    "# For now, we will remove all non UTF-8 characters from the raw data\n",
    "# We handle this by removing all � characters from the raw data and writing the modified content back to the file.\n",
    "\n",
    "def fix_non_utf8_encoding(filepath, destination_filepath):\n",
    "    with open(filepath, 'rb') as file:\n",
    "        rawdata = file.read()\n",
    "        result = chardet.detect(rawdata)\n",
    "        print(result['encoding'])\n",
    "\n",
    "\n",
    "    # Open the file in read mode, read its contents, then close it\n",
    "    with open('data/disaster-tweets.csv', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Remove all � characters\n",
    "    content = content.replace('�', '')\n",
    "\n",
    "    # Open the file in write mode and write the modified content back to it\n",
    "    with open(destination_filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "filepath = 'data/disaster-tweets.csv'\n",
    "dest = 'data/disaster-tweets-utf8.csv'\n",
    "\n",
    "# fix_non_utf8_encoding(filepath, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(filepath):\n",
    "    df = pd.read_csv(filepath, encoding='utf-8')\n",
    "    train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "    return train_data, test_data\n",
    "\n",
    "filepath = 'data/disaster-tweets-utf8.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>choose_one:confidence</th>\n",
       "      <th>choose_one_gold</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>778253309</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 16:07</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>screamed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i dont even remember slsp happening i just rem...</td>\n",
       "      <td>6.291070e+17</td>\n",
       "      <td>2.327739e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>778251995</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 20:16</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mudslide</td>\n",
       "      <td>Edinburgh</td>\n",
       "      <td>@hazelannmac ooh now I feel guilty about wishi...</td>\n",
       "      <td>6.290180e+17</td>\n",
       "      <td>2.750220e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>778247239</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/30/15 0:15</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>collide</td>\n",
       "      <td>planeta H2o</td>\n",
       "      <td>Soultech - Collide (Club Mix) http://t.co/8xIx...</td>\n",
       "      <td>6.290920e+17</td>\n",
       "      <td>6.052387e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>778255430</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 17:03</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.7978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wounded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police Officer Wounded Suspect Dead After Exch...</td>\n",
       "      <td>6.291190e+17</td>\n",
       "      <td>2.305930e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778255609</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 22:11</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Sunny Southern California</td>\n",
       "      <td>Cramer: Iger's 3 words that wrecked Disney's s...</td>\n",
       "      <td>6.290800e+17</td>\n",
       "      <td>2.464266e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  778253309    False   finalized                   5     8/27/15 16:07   \n",
       "1  778251995    False   finalized                   5     8/27/15 20:16   \n",
       "2  778247239    False   finalized                   5      8/30/15 0:15   \n",
       "3  778255430    False   finalized                   5     8/27/15 17:03   \n",
       "4  778255609    False   finalized                   5     8/27/15 22:11   \n",
       "\n",
       "     choose_one  choose_one:confidence choose_one_gold   keyword  \\\n",
       "0  Not Relevant                 1.0000             NaN  screamed   \n",
       "1  Not Relevant                 1.0000             NaN  mudslide   \n",
       "2  Not Relevant                 1.0000             NaN   collide   \n",
       "3      Relevant                 0.7978             NaN   wounded   \n",
       "4  Not Relevant                 1.0000             NaN   wrecked   \n",
       "\n",
       "                    location  \\\n",
       "0                        NaN   \n",
       "1                  Edinburgh   \n",
       "2                planeta H2o   \n",
       "3                        NaN   \n",
       "4  Sunny Southern California   \n",
       "\n",
       "                                                text       tweetid  \\\n",
       "0  i dont even remember slsp happening i just rem...  6.291070e+17   \n",
       "1  @hazelannmac ooh now I feel guilty about wishi...  6.290180e+17   \n",
       "2  Soultech - Collide (Club Mix) http://t.co/8xIx...  6.290920e+17   \n",
       "3  Police Officer Wounded Suspect Dead After Exch...  6.291190e+17   \n",
       "4  Cramer: Iger's 3 words that wrecked Disney's s...  6.290800e+17   \n",
       "\n",
       "         userid  \n",
       "0  2.327739e+08  \n",
       "1  2.750220e+07  \n",
       "2  6.052387e+08  \n",
       "3  2.305930e+09  \n",
       "4  2.464266e+07  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import_remote = False\n",
    "\n",
    "if import_remote:\n",
    "    df_train = pd.read_csv('https://raw.githubusercontent.com/magnusrodseth/it3212/main/data/train.csv', encoding='utf-8')\n",
    "    df_test = pd.read_csv('https://raw.githubusercontent.com/magnusrodseth/it3212/main/data/test.csv', encoding='utf-8')\n",
    "else:\n",
    "    df_train, df_test = split_train_test(filepath)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>choose_one:confidence</th>\n",
       "      <th>choose_one_gold</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6850</th>\n",
       "      <td>778249699</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>9/1/15 4:45</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.6084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>Washington, D.C.</td>\n",
       "      <td>Kendall Jenner and Nick Jonas Are Dating and the World Might Quite Literally Explode http://t.co/pfvzVPxQGr</td>\n",
       "      <td>6.290870e+17</td>\n",
       "      <td>3.346248e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3807</th>\n",
       "      <td>778249678</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>9/1/15 15:45</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.6024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>Pea Ridge, WV</td>\n",
       "      <td>@wyattmccab you'd throw a can of Copenhagen wintergreen on the ground that would explode on your enemies and give them mouth cancer</td>\n",
       "      <td>6.290920e+17</td>\n",
       "      <td>2.654272e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4203</th>\n",
       "      <td>778249703</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/29/15 12:05</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.8001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>im sooooooo full my stomach is going to explode</td>\n",
       "      <td>6.290840e+17</td>\n",
       "      <td>2.759442e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6725</th>\n",
       "      <td>778249712</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/28/15 20:10</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.6030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@allen_enbot If you mess up it's gonna explode...</td>\n",
       "      <td>6.290930e+17</td>\n",
       "      <td>2.904779e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>778249685</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/31/15 3:17</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.8012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>London / Berlin / Online</td>\n",
       "      <td>'I eat because it makes my mouth explode with joy and my soul rise upwards.' ~ http://t.co/mOdM8X1Ot9 http://t.co/oSsC7Q12iR</td>\n",
       "      <td>6.290790e+17</td>\n",
       "      <td>2.316803e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3954</th>\n",
       "      <td>778249695</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 18:10</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.7982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>Spring Grove, IL</td>\n",
       "      <td>If Schwarber ran into me going that fast I would explode into pieces</td>\n",
       "      <td>6.290800e+17</td>\n",
       "      <td>1.261018e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5447</th>\n",
       "      <td>778249705</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/29/15 9:55</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.7990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>Yamaku Academy, Class 3-4</td>\n",
       "      <td>Versions of KS where if a character was /every/ character world would explode.\\n\\nRin\\nShizune\\nMisha\\nEmi\\nKenji\\nYuuko\\nNomiya\\nHisao</td>\n",
       "      <td>6.290790e+17</td>\n",
       "      <td>2.484396e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>778249714</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/30/15 13:25</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>Cleveland, TN</td>\n",
       "      <td>VINE OF THE YEAR OH MY GOD I AM ABOUT TO EXPLODE https://t.co/cnxXmfFRae</td>\n",
       "      <td>6.290870e+17</td>\n",
       "      <td>8.545018e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6508</th>\n",
       "      <td>778249692</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/30/15 9:02</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>Yamaku Academy, Class 3-4</td>\n",
       "      <td>KS except every character is Shizune.\\nThe world would explode.</td>\n",
       "      <td>6.290790e+17</td>\n",
       "      <td>2.484396e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>778249726</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/28/15 4:29</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>@DelDryden If I press on the twitch will my head explode?</td>\n",
       "      <td>6.290840e+17</td>\n",
       "      <td>1.546260e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>778249698</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/31/15 4:41</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.5988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i swea it feels like im about to explode ??</td>\n",
       "      <td>6.290840e+17</td>\n",
       "      <td>2.318446e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7058</th>\n",
       "      <td>778249721</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>9/1/15 16:34</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.5958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>Redding</td>\n",
       "      <td>I feel like I'm going to explode with excitement! Wonder begins within the hour__ https://t.co/zDZJ5kRbzr</td>\n",
       "      <td>6.290900e+17</td>\n",
       "      <td>2.699312e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7287</th>\n",
       "      <td>778249693</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/30/15 9:02</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Whether you like it or not everything comes out of the dark be ready for that shit to explode ??</td>\n",
       "      <td>6.290790e+17</td>\n",
       "      <td>4.638993e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7574</th>\n",
       "      <td>778249690</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>6</td>\n",
       "      <td>8/27/15 16:07</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.6642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Some guys explode ??</td>\n",
       "      <td>6.290780e+17</td>\n",
       "      <td>3.057619e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7935</th>\n",
       "      <td>778249709</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/28/15 3:30</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>Kajang ? UiTM Puncak Alam</td>\n",
       "      <td>My head gonna explode soon</td>\n",
       "      <td>6.290780e+17</td>\n",
       "      <td>1.631294e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8066</th>\n",
       "      <td>778249727</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>9/1/15 13:26</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.5970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Toronto going crazy for the blue jays. Can you imagine if the leafs get good? The city might literally explode.</td>\n",
       "      <td>6.290910e+17</td>\n",
       "      <td>3.411643e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8300</th>\n",
       "      <td>778249686</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 19:39</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>Williamsburg, VA</td>\n",
       "      <td>Housing Starts Explode to NewHeights http://t.co/IGlnQpgbNW http://t.co/aOesBVns45</td>\n",
       "      <td>6.290900e+17</td>\n",
       "      <td>1.974801e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8394</th>\n",
       "      <td>778249710</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 13:42</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>@deniseromano @megynkelly @GOP That's one way to make their heads explode...</td>\n",
       "      <td>6.290790e+17</td>\n",
       "      <td>1.800245e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>778249708</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>9/2/15 6:32</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Learn How I Gained Access To The Secrets Of The Top Earners &amp;amp; Used Them To Explode My Home Business Here: http://t.co/8rABhQrTh5 Please #RT</td>\n",
       "      <td>6.290840e+17</td>\n",
       "      <td>2.809291e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3433</th>\n",
       "      <td>778249683</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 17:44</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Learn How I Gained Access To The Secrets Of The Top Earners &amp;amp; Used Them To Explode My Home Business Here: http://t.co/SGXP1U5OL1 Please #RT</td>\n",
       "      <td>6.290840e+17</td>\n",
       "      <td>2.808061e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>778249723</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/29/15 2:40</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My head is gonna explode</td>\n",
       "      <td>6.290790e+17</td>\n",
       "      <td>3.892895e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3319</th>\n",
       "      <td>778249722</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/28/15 3:54</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Anonchimp think its a tie with thunderstorms tho they make my soul explode...</td>\n",
       "      <td>6.290860e+17</td>\n",
       "      <td>7.727584e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>778249719</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>9/1/15 16:59</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.5946</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I need a follow before I explode @GraysonDolan</td>\n",
       "      <td>6.290890e+17</td>\n",
       "      <td>3.041554e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>778249694</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 17:10</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>Australia</td>\n",
       "      <td>It's cold and my head wants to explode.. The joys of working from home - I'm going back to bed / peace out ????</td>\n",
       "      <td>6.290830e+17</td>\n",
       "      <td>2.708602e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>778249713</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/30/15 18:33</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>? Philly Baby ?</td>\n",
       "      <td>My brains going to explode i need to leave this house. Ill be out smoking packs if you need me</td>\n",
       "      <td>6.290930e+17</td>\n",
       "      <td>3.425250e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>778249717</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 21:03</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>my deli</td>\n",
       "      <td>what if i want to fuck the duck until explode. it could be greasy</td>\n",
       "      <td>6.290840e+17</td>\n",
       "      <td>3.154441e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>778249697</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 17:20</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.7994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@magicallester I will die. I'm actually being serious. My heart will beat so fast it will fly out off my chest &amp;amp; explode</td>\n",
       "      <td>6.290860e+17</td>\n",
       "      <td>3.308588e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>778249711</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/28/15 17:13</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.7994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Block the plate with a charging Schwarber coming down the line Cervelli. I dare you. You would explode into a little puff of smoke</td>\n",
       "      <td>6.290810e+17</td>\n",
       "      <td>1.617821e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>778249680</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/30/15 5:51</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>sam</td>\n",
       "      <td>happy Justin makes my heart explode</td>\n",
       "      <td>6.290840e+17</td>\n",
       "      <td>2.714475e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>778249681</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>9/1/15 15:49</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.7982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>|IG: imaginedragoner</td>\n",
       "      <td>If Ryan doesn't release new music soon I might explode</td>\n",
       "      <td>6.290930e+17</td>\n",
       "      <td>1.703209e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>778249679</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>9/1/15 16:11</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.5958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my damn head feel like it's gone explode ??</td>\n",
       "      <td>6.290750e+17</td>\n",
       "      <td>2.507798e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>778249716</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 21:03</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Learn How I Gained Access To The Secrets Of The Top Earners &amp;amp; Used Them To Explode My Home Business Here: http://t.co/e84IFMCczN Please #RT</td>\n",
       "      <td>6.290840e+17</td>\n",
       "      <td>2.808698e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2361</th>\n",
       "      <td>778249701</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/30/15 9:02</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>Bloomington, IN</td>\n",
       "      <td>After having two cans explode I wanted to drink the rest but these ... (Kaldi Coffee Stout) http://t.co/u6isXv2F3V #photo</td>\n",
       "      <td>6.290790e+17</td>\n",
       "      <td>1.158837e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>778249718</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>6</td>\n",
       "      <td>8/27/15 13:46</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.8332</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All these people explode ????</td>\n",
       "      <td>6.290860e+17</td>\n",
       "      <td>3.089873e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2846</th>\n",
       "      <td>778249682</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>6</td>\n",
       "      <td>8/27/15 13:46</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>~always in motion~</td>\n",
       "      <td>Vanessa was about to explode! This is what she wanted to say to Shelli. Their alliance will survive. #BB17 #BBLF http://t.co/rypGKScHng</td>\n",
       "      <td>6.290840e+17</td>\n",
       "      <td>1.717552e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3135</th>\n",
       "      <td>778249700</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>9/1/15 13:41</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>Oklahoma City, OK</td>\n",
       "      <td>my brain id about to explode lmao</td>\n",
       "      <td>6.290880e+17</td>\n",
       "      <td>1.564928e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3158</th>\n",
       "      <td>778249720</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/29/15 10:00</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Learn How I Gained Access To The Secrets Of The Top Earners &amp;amp; Used Them To Explode My Home Business Here: http://t.co/dHaMbP54Ya Please #RT</td>\n",
       "      <td>6.290840e+17</td>\n",
       "      <td>2.809208e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8612</th>\n",
       "      <td>778249704</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>9/1/15 13:34</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>explode</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Learn How I Gained Access To The Secrets Of The Top Earners &amp;amp; Used Them To Explode My Home Business Here: http://t.co/UcLVIhwOEC Please #RT</td>\n",
       "      <td>6.290840e+17</td>\n",
       "      <td>2.808797e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "6850  778249699    False   finalized                   5       9/1/15 4:45   \n",
       "3807  778249678    False   finalized                   5      9/1/15 15:45   \n",
       "4203  778249703    False   finalized                   5     8/29/15 12:05   \n",
       "6725  778249712    False   finalized                   5     8/28/15 20:10   \n",
       "81    778249685    False   finalized                   5      8/31/15 3:17   \n",
       "3954  778249695    False   finalized                   5     8/27/15 18:10   \n",
       "5447  778249705    False   finalized                   5      8/29/15 9:55   \n",
       "5620  778249714    False   finalized                   5     8/30/15 13:25   \n",
       "6508  778249692    False   finalized                   5      8/30/15 9:02   \n",
       "6915  778249726    False   finalized                   5      8/28/15 4:29   \n",
       "389   778249698    False   finalized                   5      8/31/15 4:41   \n",
       "7058  778249721    False   finalized                   5      9/1/15 16:34   \n",
       "7287  778249693    False   finalized                   5      8/30/15 9:02   \n",
       "7574  778249690    False   finalized                   6     8/27/15 16:07   \n",
       "7935  778249709    False   finalized                   5      8/28/15 3:30   \n",
       "8066  778249727    False   finalized                   5      9/1/15 13:26   \n",
       "8300  778249686    False   finalized                   5     8/27/15 19:39   \n",
       "8394  778249710    False   finalized                   5     8/27/15 13:42   \n",
       "3661  778249708    False   finalized                   5       9/2/15 6:32   \n",
       "3433  778249683    False   finalized                   5     8/27/15 17:44   \n",
       "3379  778249723    False   finalized                   5      8/29/15 2:40   \n",
       "3319  778249722    False   finalized                   5      8/28/15 3:54   \n",
       "641   778249719    False   finalized                   5      9/1/15 16:59   \n",
       "670   778249694    False   finalized                   5     8/27/15 17:10   \n",
       "728   778249713    False   finalized                   5     8/30/15 18:33   \n",
       "918   778249717    False   finalized                   5     8/27/15 21:03   \n",
       "1160  778249697    False   finalized                   5     8/27/15 17:20   \n",
       "1296  778249711    False   finalized                   5     8/28/15 17:13   \n",
       "1348  778249680    False   finalized                   5      8/30/15 5:51   \n",
       "1950  778249681    False   finalized                   5      9/1/15 15:49   \n",
       "2021  778249679    False   finalized                   5      9/1/15 16:11   \n",
       "2131  778249716    False   finalized                   5     8/27/15 21:03   \n",
       "2361  778249701    False   finalized                   5      8/30/15 9:02   \n",
       "2605  778249718    False   finalized                   6     8/27/15 13:46   \n",
       "2846  778249682    False   finalized                   6     8/27/15 13:46   \n",
       "3135  778249700    False   finalized                   5      9/1/15 13:41   \n",
       "3158  778249720    False   finalized                   5     8/29/15 10:00   \n",
       "8612  778249704    False   finalized                   5      9/1/15 13:34   \n",
       "\n",
       "        choose_one  choose_one:confidence choose_one_gold  keyword  \\\n",
       "6850      Relevant                 0.6084             NaN  explode   \n",
       "3807      Relevant                 0.6024             NaN  explode   \n",
       "4203      Relevant                 0.8001             NaN  explode   \n",
       "6725      Relevant                 0.6030             NaN  explode   \n",
       "81    Not Relevant                 0.8012             NaN  explode   \n",
       "3954  Not Relevant                 0.7982             NaN  explode   \n",
       "5447  Not Relevant                 0.7990             NaN  explode   \n",
       "5620  Not Relevant                 1.0000             NaN  explode   \n",
       "6508  Not Relevant                 1.0000             NaN  explode   \n",
       "6915  Not Relevant                 1.0000             NaN  explode   \n",
       "389   Not Relevant                 0.5988             NaN  explode   \n",
       "7058  Not Relevant                 0.5958             NaN  explode   \n",
       "7287  Not Relevant                 1.0000             NaN  explode   \n",
       "7574  Not Relevant                 0.6642             NaN  explode   \n",
       "7935  Not Relevant                 1.0000             NaN  explode   \n",
       "8066  Not Relevant                 0.5970             NaN  explode   \n",
       "8300  Not Relevant                 1.0000             NaN  explode   \n",
       "8394  Not Relevant                 1.0000             NaN  explode   \n",
       "3661  Not Relevant                 1.0000             NaN  explode   \n",
       "3433  Not Relevant                 1.0000             NaN  explode   \n",
       "3379  Not Relevant                 1.0000             NaN  explode   \n",
       "3319  Not Relevant                 0.7988             NaN  explode   \n",
       "641   Not Relevant                 0.5946             NaN  explode   \n",
       "670   Not Relevant                 1.0000             NaN  explode   \n",
       "728   Not Relevant                 0.7988             NaN  explode   \n",
       "918   Not Relevant                 1.0000             NaN  explode   \n",
       "1160  Not Relevant                 0.7994             NaN  explode   \n",
       "1296  Not Relevant                 0.7994             NaN  explode   \n",
       "1348  Not Relevant                 1.0000             NaN  explode   \n",
       "1950  Not Relevant                 0.7982             NaN  explode   \n",
       "2021  Not Relevant                 0.5958             NaN  explode   \n",
       "2131  Not Relevant                 1.0000             NaN  explode   \n",
       "2361  Not Relevant                 1.0000             NaN  explode   \n",
       "2605  Not Relevant                 0.8332             NaN  explode   \n",
       "2846  Not Relevant                 1.0000             NaN  explode   \n",
       "3135  Not Relevant                 1.0000             NaN  explode   \n",
       "3158  Not Relevant                 0.5994             NaN  explode   \n",
       "8612  Not Relevant                 1.0000             NaN  explode   \n",
       "\n",
       "                       location  \\\n",
       "6850           Washington, D.C.   \n",
       "3807              Pea Ridge, WV   \n",
       "4203                        NaN   \n",
       "6725                        NaN   \n",
       "81     London / Berlin / Online   \n",
       "3954           Spring Grove, IL   \n",
       "5447  Yamaku Academy, Class 3-4   \n",
       "5620              Cleveland, TN   \n",
       "6508  Yamaku Academy, Class 3-4   \n",
       "6915              New Hampshire   \n",
       "389                         NaN   \n",
       "7058                   Redding    \n",
       "7287                        NaN   \n",
       "7574                        NaN   \n",
       "7935  Kajang ? UiTM Puncak Alam   \n",
       "8066                        NaN   \n",
       "8300           Williamsburg, VA   \n",
       "8394                 Dallas, TX   \n",
       "3661                        NaN   \n",
       "3433                        NaN   \n",
       "3379                        NaN   \n",
       "3319                        NaN   \n",
       "641                         NaN   \n",
       "670                   Australia   \n",
       "728             ? Philly Baby ?   \n",
       "918                     my deli   \n",
       "1160                        NaN   \n",
       "1296                        NaN   \n",
       "1348                        sam   \n",
       "1950       |IG: imaginedragoner   \n",
       "2021                        NaN   \n",
       "2131                        NaN   \n",
       "2361            Bloomington, IN   \n",
       "2605                        NaN   \n",
       "2846         ~always in motion~   \n",
       "3135          Oklahoma City, OK   \n",
       "3158                        NaN   \n",
       "8612                        NaN   \n",
       "\n",
       "                                                                                                                                                 text  \\\n",
       "6850                                      Kendall Jenner and Nick Jonas Are Dating and the World Might Quite Literally Explode http://t.co/pfvzVPxQGr   \n",
       "3807              @wyattmccab you'd throw a can of Copenhagen wintergreen on the ground that would explode on your enemies and give them mouth cancer   \n",
       "4203                                                                                                  im sooooooo full my stomach is going to explode   \n",
       "6725                                                                                                @allen_enbot If you mess up it's gonna explode...   \n",
       "81                       'I eat because it makes my mouth explode with joy and my soul rise upwards.' ~ http://t.co/mOdM8X1Ot9 http://t.co/oSsC7Q12iR   \n",
       "3954                                                                             If Schwarber ran into me going that fast I would explode into pieces   \n",
       "5447          Versions of KS where if a character was /every/ character world would explode.\\n\\nRin\\nShizune\\nMisha\\nEmi\\nKenji\\nYuuko\\nNomiya\\nHisao   \n",
       "5620                                                                         VINE OF THE YEAR OH MY GOD I AM ABOUT TO EXPLODE https://t.co/cnxXmfFRae   \n",
       "6508                                                                                  KS except every character is Shizune.\\nThe world would explode.   \n",
       "6915                                                                                        @DelDryden If I press on the twitch will my head explode?   \n",
       "389                                                                                                       i swea it feels like im about to explode ??   \n",
       "7058                                        I feel like I'm going to explode with excitement! Wonder begins within the hour__ https://t.co/zDZJ5kRbzr   \n",
       "7287                                                 Whether you like it or not everything comes out of the dark be ready for that shit to explode ??   \n",
       "7574                                                                                                                             Some guys explode ??   \n",
       "7935                                                                                                                       My head gonna explode soon   \n",
       "8066                                  Toronto going crazy for the blue jays. Can you imagine if the leafs get good? The city might literally explode.   \n",
       "8300                                                               Housing Starts Explode to NewHeights http://t.co/IGlnQpgbNW http://t.co/aOesBVns45   \n",
       "8394                                                                     @deniseromano @megynkelly @GOP That's one way to make their heads explode...   \n",
       "3661  Learn How I Gained Access To The Secrets Of The Top Earners &amp; Used Them To Explode My Home Business Here: http://t.co/8rABhQrTh5 Please #RT   \n",
       "3433  Learn How I Gained Access To The Secrets Of The Top Earners &amp; Used Them To Explode My Home Business Here: http://t.co/SGXP1U5OL1 Please #RT   \n",
       "3379                                                                                                                         My head is gonna explode   \n",
       "3319                                                                   @Anonchimp think its a tie with thunderstorms tho they make my soul explode...   \n",
       "641                                                                                                    I need a follow before I explode @GraysonDolan   \n",
       "670                                   It's cold and my head wants to explode.. The joys of working from home - I'm going back to bed / peace out ????   \n",
       "728                                                    My brains going to explode i need to leave this house. Ill be out smoking packs if you need me   \n",
       "918                                                                                 what if i want to fuck the duck until explode. it could be greasy   \n",
       "1160                     @magicallester I will die. I'm actually being serious. My heart will beat so fast it will fly out off my chest &amp; explode   \n",
       "1296               Block the plate with a charging Schwarber coming down the line Cervelli. I dare you. You would explode into a little puff of smoke   \n",
       "1348                                                                                                              happy Justin makes my heart explode   \n",
       "1950                                                                                           If Ryan doesn't release new music soon I might explode   \n",
       "2021                                                                                                      my damn head feel like it's gone explode ??   \n",
       "2131  Learn How I Gained Access To The Secrets Of The Top Earners &amp; Used Them To Explode My Home Business Here: http://t.co/e84IFMCczN Please #RT   \n",
       "2361                        After having two cans explode I wanted to drink the rest but these ... (Kaldi Coffee Stout) http://t.co/u6isXv2F3V #photo   \n",
       "2605                                                                                                                    All these people explode ????   \n",
       "2846          Vanessa was about to explode! This is what she wanted to say to Shelli. Their alliance will survive. #BB17 #BBLF http://t.co/rypGKScHng   \n",
       "3135                                                                                                                my brain id about to explode lmao   \n",
       "3158  Learn How I Gained Access To The Secrets Of The Top Earners &amp; Used Them To Explode My Home Business Here: http://t.co/dHaMbP54Ya Please #RT   \n",
       "8612  Learn How I Gained Access To The Secrets Of The Top Earners &amp; Used Them To Explode My Home Business Here: http://t.co/UcLVIhwOEC Please #RT   \n",
       "\n",
       "           tweetid        userid  \n",
       "6850  6.290870e+17  3.346248e+08  \n",
       "3807  6.290920e+17  2.654272e+08  \n",
       "4203  6.290840e+17  2.759442e+09  \n",
       "6725  6.290930e+17  2.904779e+08  \n",
       "81    6.290790e+17  2.316803e+09  \n",
       "3954  6.290800e+17  1.261018e+09  \n",
       "5447  6.290790e+17  2.484396e+09  \n",
       "5620  6.290870e+17  8.545018e+08  \n",
       "6508  6.290790e+17  2.484396e+09  \n",
       "6915  6.290840e+17  1.546260e+07  \n",
       "389   6.290840e+17  2.318446e+09  \n",
       "7058  6.290900e+17  2.699312e+08  \n",
       "7287  6.290790e+17  4.638993e+08  \n",
       "7574  6.290780e+17  3.057619e+09  \n",
       "7935  6.290780e+17  1.631294e+09  \n",
       "8066  6.290910e+17  3.411643e+08  \n",
       "8300  6.290900e+17  1.974801e+08  \n",
       "8394  6.290790e+17  1.800245e+07  \n",
       "3661  6.290840e+17  2.809291e+09  \n",
       "3433  6.290840e+17  2.808061e+09  \n",
       "3379  6.290790e+17  3.892895e+08  \n",
       "3319  6.290860e+17  7.727584e+07  \n",
       "641   6.290890e+17  3.041554e+08  \n",
       "670   6.290830e+17  2.708602e+08  \n",
       "728   6.290930e+17  3.425250e+08  \n",
       "918   6.290840e+17  3.154441e+08  \n",
       "1160  6.290860e+17  3.308588e+09  \n",
       "1296  6.290810e+17  1.617821e+08  \n",
       "1348  6.290840e+17  2.714475e+09  \n",
       "1950  6.290930e+17  1.703209e+09  \n",
       "2021  6.290750e+17  2.507798e+09  \n",
       "2131  6.290840e+17  2.808698e+09  \n",
       "2361  6.290790e+17  1.158837e+09  \n",
       "2605  6.290860e+17  3.089873e+09  \n",
       "2846  6.290840e+17  1.717552e+08  \n",
       "3135  6.290880e+17  1.564928e+09  \n",
       "3158  6.290840e+17  2.809208e+09  \n",
       "8612  6.290840e+17  2.808797e+09  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all rows where keyword is 'wreckagec\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_train[df_train['keyword'] == 'explode'].sort_values(by='choose_one', ascending=False).head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    # Clean `keyword` column.\n",
    "\n",
    "    # Write the updated dataframe to a new CSV file\n",
    "    # Plot the most common keywords\n",
    "    defined_keywords = df_train[df_train['keyword'] != '']['keyword']\n",
    "\n",
    "    plt.figure()\n",
    "    sns.countplot(y=defined_keywords, order=defined_keywords.value_counts().iloc[:10].index)\n",
    "    plt.title('Most Common Keywords')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Keyword')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    # Compare keywords for disaster tweets and non-disaster tweets\n",
    "    disaster_keywords = df_train[df_train['choose_one'] == 'Relevant']['keyword']\n",
    "    non_disaster_keywords = df_train[df_train['choose_one'] == 'Not Relevant']['keyword']\n",
    "\n",
    "    # Create a figure object and define the grid\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))  # 1 row, 2 columns\n",
    "\n",
    "    # Plotting\n",
    "    sns.countplot(y=disaster_keywords, ax=ax[0], order=disaster_keywords.value_counts().iloc[:10].index, color='red')\n",
    "    sns.countplot(y=non_disaster_keywords, ax=ax[1], order=non_disaster_keywords.value_counts().iloc[:10].index, color='blue')\n",
    "\n",
    "    # Titles and labels\n",
    "    ax[0].set_title('Most Common Keywords for Disaster Tweets')\n",
    "    ax[0].set_xlabel('Count')\n",
    "    ax[0].set_ylabel('Keyword')\n",
    "\n",
    "    ax[1].set_title('Most Common Keywords for Non-Disaster Tweets')\n",
    "    ax[1].set_xlabel('Count')\n",
    "    ax[1].set_ylabel('Keyword')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, we can see that the top 10 shared keywords of disaster-related tweets and non-disaster-related tweets do not share any common keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 2167 of total: 8700 rows. Remaining rows: 6533\n",
      "Removed 635 duplicated rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sunset looked like erupting volcano initial thought pixar short lava</td>\n",
       "      <td>volcano</td>\n",
       "      <td>The sunset looked like an erupting volcano .... My initial thought was the Pixar short Lava http://t.co/g4sChqFEsT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7294 nikon d50 61 mp digital slr camera body 2 batteries carry bag charger 20000</td>\n",
       "      <td>body bag</td>\n",
       "      <td>#7294 Nikon D50 6.1 MP Digital SLR Camera Body 2 batteries carry bag and charger http://t.co/SL7PHqSGKV\\n\\n$200.00\\n_ http://t.co/T4Qh2OM8Op</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>mentaltwitter note make sure smoke alarm battery snuff times face many twitter reminders changing battery</td>\n",
       "      <td>smoke</td>\n",
       "      <td>Mental/Twitter Note: Make sure my smoke alarm battery is up to snuff at all times or face many twitter reminders of changing my battery.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>emergency need part 2 3 nashnewvideo nashgrier 103</td>\n",
       "      <td>emergency</td>\n",
       "      <td>?????? EMERGENCY ?????? NEED PART 2 and 3!!! #NashNewVideo http://t.co/TwdnNaIOns @Nashgrier 103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>whelen model 295ss100 siren amplifier police emergency vehicle full read ebay</td>\n",
       "      <td>siren</td>\n",
       "      <td>WHELEN MODEL 295SS-100 SIREN AMPLIFIER POLICE EMERGENCY VEHICLE - Full read by eBay http://t.co/Q3yYQi4A27 http://t.co/whEreofYAx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  \\\n",
       "0       1   \n",
       "1       1   \n",
       "2       0   \n",
       "3       0   \n",
       "4       0   \n",
       "\n",
       "                                                                                                        text  \\\n",
       "0                                       sunset looked like erupting volcano initial thought pixar short lava   \n",
       "1                           7294 nikon d50 61 mp digital slr camera body 2 batteries carry bag charger 20000   \n",
       "2  mentaltwitter note make sure smoke alarm battery snuff times face many twitter reminders changing battery   \n",
       "3                                                         emergency need part 2 3 nashnewvideo nashgrier 103   \n",
       "4                              whelen model 295ss100 siren amplifier police emergency vehicle full read ebay   \n",
       "\n",
       "     keyword  \\\n",
       "0    volcano   \n",
       "1   body bag   \n",
       "2      smoke   \n",
       "3  emergency   \n",
       "4      siren   \n",
       "\n",
       "                                                                                                                                       text_raw  \n",
       "0                            The sunset looked like an erupting volcano .... My initial thought was the Pixar short Lava http://t.co/g4sChqFEsT  \n",
       "1  #7294 Nikon D50 6.1 MP Digital SLR Camera Body 2 batteries carry bag and charger http://t.co/SL7PHqSGKV\\n\\n$200.00\\n_ http://t.co/T4Qh2OM8Op  \n",
       "2      Mental/Twitter Note: Make sure my smoke alarm battery is up to snuff at all times or face many twitter reminders of changing my battery.  \n",
       "3                                              ?????? EMERGENCY ?????? NEED PART 2 and 3!!! #NashNewVideo http://t.co/TwdnNaIOns @Nashgrier 103  \n",
       "4             WHELEN MODEL 295SS-100 SIREN AMPLIFIER POLICE EMERGENCY VEHICLE - Full read by eBay http://t.co/Q3yYQi4A27 http://t.co/whEreofYAx  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def filter_rows_by_confidence_and_decision(df, confidence_threshold):\n",
    "    df = df[df['choose_one:confidence'] >= confidence_threshold]\n",
    "    df = df[df['choose_one'] != \"Can't Decide\"]\n",
    "    return df\n",
    "\n",
    "def map_choose_one_to_y(df):\n",
    "    df['target'] = df['choose_one'].apply(lambda choice: 1 if choice == 'Relevant' else 0)\n",
    "    return df\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words(\"english\")])\n",
    "    if lemmatize:\n",
    "        text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    text = contractions.fix(text)\n",
    "    text = ' '.join(tokenizer.tokenize(text))\n",
    "    return text\n",
    "\n",
    "def clean_keyword(keyword):\n",
    "    return unquote(keyword) if pd.notnull(keyword) else ''\n",
    "\n",
    "def clean_data(df):\n",
    "    df['keyword'] = df['keyword'].apply(clean_keyword).apply(str.lower)\n",
    "    df['text_raw'] = df['text']\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "initial_count = df_train.shape[0]\n",
    "confidence_threshold = 0.7\n",
    "\n",
    "df_train = filter_rows_by_confidence_and_decision(df_train, confidence_threshold)\n",
    "print(\"Removed {} of total: {} rows. Remaining rows: {}\".format(initial_count - df_train.shape[0], initial_count, df_train.shape[0]))\n",
    "\n",
    "features_to_keep = ['target', 'text', 'keyword']\n",
    "\n",
    "df_train = map_choose_one_to_y(df_train)\n",
    "df_train = df_train[features_to_keep]\n",
    "df_train = clean_data(df_train)\n",
    "\n",
    "count_initial = df_train.shape[0]\n",
    "df_train = df_train.drop_duplicates(subset=['text'])\n",
    "print(\"Removed {} duplicated rows.\".format(count_initial - df_train.shape[0]))\n",
    "\n",
    "\n",
    "# Preprocess the test data as well\n",
    "df_test = map_choose_one_to_y(df_test)\n",
    "df_test = df_test[features_to_keep]\n",
    "df_test = clean_data(df_test)\n",
    "\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellanous features from `text` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text_raw</th>\n",
       "      <th>text_length</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>has_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>do not even remember slsp happening remember like wtf lights turned everyone screamed encore</td>\n",
       "      <td>screamed</td>\n",
       "      <td>i dont even remember slsp happening i just remember being like wtf and then the lights turned off and everyone screamed for the encore</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>hazelannmac ooh feel guilty wishing hatman bet mudslide delicious</td>\n",
       "      <td>mudslide</td>\n",
       "      <td>@hazelannmac ooh now I feel guilty about wishing hatman out. I bet the mudslide was delicious!</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>soultech collide club mix</td>\n",
       "      <td>collide</td>\n",
       "      <td>Soultech - Collide (Club Mix) http://t.co/8xIxBsPOT8</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>police officer wounded suspect dead exchanging shots</td>\n",
       "      <td>wounded</td>\n",
       "      <td>Police Officer Wounded Suspect Dead After Exchanging Shots - http://t.co/iPHaZV47g7</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>cramer igers 3 words wrecked disneys stock</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Cramer: Iger's 3 words that wrecked Disney's stock http://t.co/4dGpBAiVL7</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  \\\n",
       "0       0   \n",
       "1       0   \n",
       "2       0   \n",
       "3       1   \n",
       "4       0   \n",
       "\n",
       "                                                                                           text  \\\n",
       "0  do not even remember slsp happening remember like wtf lights turned everyone screamed encore   \n",
       "1                             hazelannmac ooh feel guilty wishing hatman bet mudslide delicious   \n",
       "2                                                                     soultech collide club mix   \n",
       "3                                          police officer wounded suspect dead exchanging shots   \n",
       "4                                                    cramer igers 3 words wrecked disneys stock   \n",
       "\n",
       "    keyword  \\\n",
       "0  screamed   \n",
       "1  mudslide   \n",
       "2   collide   \n",
       "3   wounded   \n",
       "4   wrecked   \n",
       "\n",
       "                                                                                                                                 text_raw  \\\n",
       "0  i dont even remember slsp happening i just remember being like wtf and then the lights turned off and everyone screamed for the encore   \n",
       "1                                          @hazelannmac ooh now I feel guilty about wishing hatman out. I bet the mudslide was delicious!   \n",
       "2                                                                                    Soultech - Collide (Club Mix) http://t.co/8xIxBsPOT8   \n",
       "3                                                     Police Officer Wounded Suspect Dead After Exchanging Shots - http://t.co/iPHaZV47g7   \n",
       "4                                                               Cramer: Iger's 3 words that wrecked Disney's stock http://t.co/4dGpBAiVL7   \n",
       "\n",
       "   text_length  hashtag_count  mention_count  has_url  \n",
       "0          134              0              0        0  \n",
       "1           94              0              1        0  \n",
       "2           52              0              0        1  \n",
       "3           83              0              0        1  \n",
       "4           73              0              0        1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features(df): \n",
    "    # Create new column for text length\n",
    "    df['text_length'] = df['text_raw'].apply(len)\n",
    "    # Extract the number of hashtags\n",
    "    df[\"hashtag_count\"] = df[\"text_raw\"].apply(lambda x: len([c for c in str(x) if c == \"#\"]))\n",
    "\n",
    "    # Extract the number of mentions\n",
    "    df[\"mention_count\"] = df[\"text_raw\"].apply(lambda x: len([c for c in str(x) if c == \"@\"]))\n",
    "\n",
    "    # Extract the `has_url` feature\n",
    "    df[\"has_url\"] = df[\"text_raw\"].apply(lambda x: 1 if \"http\" in str(x) else 0)\n",
    "    return df\n",
    "\n",
    "# Write the updated dataframe to a CSV file\n",
    "df_train = extract_features(df_train)\n",
    "df_test = extract_features(df_test)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_with_ngrams</th>\n",
       "      <th>text_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do not even remember slsp happening remember like wtf lights turned everyone screamed encore do_not not_even even_remember remember_slsp slsp_happening happening_remember remember_like like_wtf wtf_lights lights_turned turned_everyone everyone_screamed screamed_encore do_not_even not_even_remember even_remember_slsp remember_slsp_happening slsp_happening_remember happening_remember_like remember_like_wtf like_wtf_lights wtf_lights_turned lights_turned_everyone turned_everyone_screamed everyone_screamed_encore</td>\n",
       "      <td>i dont even remember slsp happening i just remember being like wtf and then the lights turned off and everyone screamed for the encore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hazelannmac ooh feel guilty wishing hatman bet mudslide delicious hazelannmac_ooh ooh_feel feel_guilty guilty_wishing wishing_hatman hatman_bet bet_mudslide mudslide_delicious hazelannmac_ooh_feel ooh_feel_guilty feel_guilty_wishing guilty_wishing_hatman wishing_hatman_bet hatman_bet_mudslide bet_mudslide_delicious</td>\n",
       "      <td>@hazelannmac ooh now I feel guilty about wishing hatman out. I bet the mudslide was delicious!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     text_with_ngrams  \\\n",
       "0  do not even remember slsp happening remember like wtf lights turned everyone screamed encore do_not not_even even_remember remember_slsp slsp_happening happening_remember remember_like like_wtf wtf_lights lights_turned turned_everyone everyone_screamed screamed_encore do_not_even not_even_remember even_remember_slsp remember_slsp_happening slsp_happening_remember happening_remember_like remember_like_wtf like_wtf_lights wtf_lights_turned lights_turned_everyone turned_everyone_screamed everyone_screamed_encore   \n",
       "1                                                                                                                                                                                                        hazelannmac ooh feel guilty wishing hatman bet mudslide delicious hazelannmac_ooh ooh_feel feel_guilty guilty_wishing wishing_hatman hatman_bet bet_mudslide mudslide_delicious hazelannmac_ooh_feel ooh_feel_guilty feel_guilty_wishing guilty_wishing_hatman wishing_hatman_bet hatman_bet_mudslide bet_mudslide_delicious   \n",
       "\n",
       "                                                                                                                                 text_raw  \n",
       "0  i dont even remember slsp happening i just remember being like wtf and then the lights turned off and everyone screamed for the encore  \n",
       "1                                          @hazelannmac ooh now I feel guilty about wishing hatman out. I bet the mudslide was delicious!  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_ngrams(text, n):\n",
    "    tokens = word_tokenize(text)\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return ['_'.join(ngram) for ngram in n_grams]\n",
    "\n",
    "def add_ngrams_to_text(text):\n",
    "    bigrams_string = ' '.join(create_ngrams(text, 2))\n",
    "    trigrams_string = ' '.join(create_ngrams(text, 3))\n",
    "    return text + ' ' + bigrams_string + ' ' + trigrams_string\n",
    "\n",
    "def add_ngrams_to_df(df):\n",
    "    df['text_with_ngrams'] = df['text'].apply(add_ngrams_to_text)\n",
    "    return df\n",
    "\n",
    "# Apply to DataFrames\n",
    "df_train = add_ngrams_to_df(df_train)\n",
    "df_test = add_ngrams_to_df(df_test)\n",
    "\n",
    "# print full rows\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_train[['text_with_ngrams', 'text_raw']].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding `text` using `TF-IDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if text_embedding['tfidf']:\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "    feature_to_embed = 'text_with_ngrams'\n",
    "\n",
    "    # Fit and transform the training data\n",
    "    text_embedded = vectorizer.fit_transform(df_train[feature_to_embed])\n",
    "    df_train_text_embedded_tfidf = pd.DataFrame(text_embedded.toarray(), columns=vectorizer.get_feature_names_out(), index=df_train.index)\n",
    "\n",
    "    # Transform the test data using the same vectorizer\n",
    "    text_embedded_test = vectorizer.transform(df_test[feature_to_embed])\n",
    "    df_test_text_embedded_tfidf = pd.DataFrame(text_embedded_test.toarray(), columns=vectorizer.get_feature_names_out(), index=df_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding `text` column using `Word2Vec`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_conifig = {\n",
    "    \"vector_size\": 200,\n",
    "    \"with_ngrams\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 758.5/758.5MB downloaded\n"
     ]
    }
   ],
   "source": [
    "if text_embedding['word2vec']:\n",
    "    if w2v_conifig['with_ngrams']:\n",
    "        tokenized_text = df_train['text_with_ngrams'].apply(lambda x: x.split())\n",
    "    else:\n",
    "        tokenized_text = df_train['text'].apply(lambda x: x.split())\n",
    "\n",
    "    import gensim.downloader as api\n",
    "    model_w2v = api.load(\"glove-twitter-200\")\n",
    "\n",
    "# model_w2v = gensim.models.Word2Vec(\n",
    "#             tokenized_text,\n",
    "#             vector_size=w2v_conifig['vector_size'], # desired no. of features/independent variables\n",
    "#             window=5, # context window size\n",
    "#             min_count=2, # Ignores all words with total frequency lower than 2.                                  \n",
    "#             sg = 1, # 1 for skip-gram model, 0 for CBOW\n",
    "#             negative = 10, # for negative sampling\n",
    "#             workers= 8, # no.of cores\n",
    "#             seed = 34\n",
    "# ) \n",
    "\n",
    "# # https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314\n",
    "# # Skip-gram: works well with a small amount of the training data, represents well even rare words or phrases.\n",
    "# # CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words.\n",
    "\n",
    "# model_w2v.train(tokenized_text, total_examples= len(df_train), epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create embeddings from `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5898, 200)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if text_embedding['word2vec']:\n",
    "    def embed_text_feature(df, col, model, vector_size):\n",
    "        def tokens_to_vectors(text_tokens) -> np.ndarray:\n",
    "            vectors = np.zeros((len(text_tokens), vector_size))\n",
    "\n",
    "            # embed each token (word-ish) in the text. If the token is not in the model's vocabulary, embed it as a zero vector.\n",
    "            for i, token in enumerate(text_tokens):\n",
    "                try:\n",
    "                    vectors[i] = model[token]\n",
    "                except KeyError:  # Token not in the model's vocabulary\n",
    "                    vectors[i] = np.zeros(vector_size)\n",
    "\n",
    "            # if all tokens were zero vectors, i.e. all words not in the model's vocabulary, return a zero vector\n",
    "            if np.all(vectors == 0):\n",
    "                return np.zeros(vector_size)\n",
    "            \n",
    "            return vectors.mean(axis=0)\n",
    "\n",
    "        embeddings = []\n",
    "        for tokens in df[col].apply(lambda x: x.split()):\n",
    "            embeddings.append(tokens_to_vectors(tokens))\n",
    "\n",
    "        return pd.DataFrame(np.vstack(embeddings), columns=[f'{col}_w2v_{i}' for i in range(vector_size)])\n",
    "\n",
    "    df_train_text_embedded_w2v = embed_text_feature(df_train, 'text', model_w2v, w2v_conifig['vector_size'])\n",
    "    df_test_text_embedded_w2v = embed_text_feature(df_test, 'text', model_w2v, w2v_conifig['vector_size'])\n",
    "\n",
    "    df_train_text_embedded_w2v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['target', 'text', 'keyword', 'text_raw', 'text_length', 'hashtag_count',\n",
       "       'mention_count', 'has_url', 'text_with_ngrams'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5898, 1004)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_keep = ['target', 'text_length', 'hashtag_count', 'mention_count', 'has_url']\n",
    "\n",
    "X_train = pd.concat([\n",
    "    df_train[features_to_keep], \n",
    "    # df_train_text_embedded_w2v,\n",
    "    df_train_text_embedded_tfidf\n",
    "    ], axis=1)\n",
    "X_test = pd.concat([\n",
    "    df_test[features_to_keep], \n",
    "    # df_test_text_embedded_w2v,\n",
    "    df_test_text_embedded_tfidf\n",
    "    ], axis=1)\n",
    "\n",
    "X_train.dropna(inplace=True)\n",
    "\n",
    "# extract y_train and y_test here to avoid column name collision with 'target' feature coming from text and keyword embeddings\n",
    "y_train = X_train['target']\n",
    "y_test = X_test['target']\n",
    "\n",
    "X_train.drop(['target'], axis=1, inplace=True)\n",
    "X_test.drop(['target'], axis=1, inplace=True)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = True\n",
    "svm = False\n",
    "xgb = True\n",
    "rforest = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(y_pred, y_train, y_pred_test, y_test):\n",
    "    print(\"Train results\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Train accuracy: {}\".format(accuracy_score(y_train, y_pred)))\n",
    "    print(classification_report(y_train, y_pred))\n",
    "    print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "    print()\n",
    "    print(\"Test results\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Test accuracy: {}\".format(accuracy_score(y_test, y_pred_test)))\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results\n",
      "-----------------------------\n",
      "Train accuracy: 0.8450322143099356\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88      3596\n",
      "           1       0.84      0.74      0.79      2302\n",
      "\n",
      "    accuracy                           0.85      5898\n",
      "   macro avg       0.84      0.83      0.83      5898\n",
      "weighted avg       0.84      0.85      0.84      5898\n",
      "\n",
      "[[3280  316]\n",
      " [ 598 1704]]\n",
      "\n",
      "Test results\n",
      "-----------------------------\n",
      "Test accuracy: 0.7867647058823529\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82      1219\n",
      "           1       0.81      0.67      0.74       957\n",
      "\n",
      "    accuracy                           0.79      2176\n",
      "   macro avg       0.79      0.77      0.78      2176\n",
      "weighted avg       0.79      0.79      0.78      2176\n",
      "\n",
      "[[1068  151]\n",
      " [ 313  644]]\n"
     ]
    }
   ],
   "source": [
    "if logreg:\n",
    "    logreg = LogisticRegression(random_state=42, solver=\"liblinear\")\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = cross_val_predict(logreg, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "    y_pred_test = logreg.predict(X_test)\n",
    "\n",
    "    print_results(y_pred, y_train, y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text_raw</th>\n",
       "      <th>text_length</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>has_url</th>\n",
       "      <th>text_with_ngrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>car crash accident explosion fire</td>\n",
       "      <td>test</td>\n",
       "      <td>car crash accident explosion fire</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>car crash accident explosion fire car_crash crash_accident accident_explosion explosion_fire car_crash_accident crash_accident_explosion accident_explosion_fire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                text keyword  \\\n",
       "0  car crash accident explosion fire    test   \n",
       "\n",
       "                            text_raw  text_length  hashtag_count  \\\n",
       "0  car crash accident explosion fire           33              0   \n",
       "\n",
       "   mention_count  has_url  \\\n",
       "0              0        0   \n",
       "\n",
       "                                                                                                                                                   text_with_ngrams  \n",
       "0  car crash accident explosion fire car_crash crash_accident accident_explosion explosion_fire car_crash_accident crash_accident_explosion accident_explosion_fire  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16828021 0.83171979]]\n"
     ]
    }
   ],
   "source": [
    "# test manual prediction\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'text': ['car crash accident explosion fire'],\n",
    "    'keyword': 'test',\n",
    "})\n",
    "\n",
    "test_df = clean_data(test_df)\n",
    "test_df = extract_features(test_df)\n",
    "test_df = add_ngrams_to_df(test_df)\n",
    "\n",
    "feature_to_embed = 'text_with_ngrams'\n",
    "vectors = vectorizer.transform(test_df[feature_to_embed])\n",
    "text_embedded = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names_out(), index=test_df.index)\n",
    "\n",
    "display(test_df)\n",
    "\n",
    "# drop text_raw and text_with_ngrams\n",
    "test_df.drop(['text_raw', 'text_with_ngrams', 'keyword', 'text'], axis=1, inplace=True)\n",
    "\n",
    "test_df = pd.concat([\n",
    "    test_df,\n",
    "    text_embedded\n",
    "    ], axis=1)\n",
    "\n",
    "y_pred_test = logreg.predict_proba(test_df)\n",
    "\n",
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if svm:\n",
    "    first_n = 1000\n",
    "\n",
    "    # Initialize SVM model\n",
    "    svm_model = SVC(kernel='linear', C=1, random_state=42, probability=False)\n",
    "\n",
    "    # Fit the model on training data\n",
    "    svm_model.fit(X_train, y_train)\n",
    "\n",
    "    # Use 5-fold cross-validation to get predictions on training set\n",
    "    y_pred_train = cross_val_predict(svm_model, X_train, y_train, cv=5)\n",
    "    y_pred_test = svm_model.predict(X_test)\n",
    "\n",
    "    print_results(y_pred_train, y_train, y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Metrics:\n",
      "Accuracy: 0.7844669117647058\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.91      0.83      1219\n",
      "           1       0.84      0.63      0.72       957\n",
      "\n",
      "    accuracy                           0.78      2176\n",
      "   macro avg       0.80      0.77      0.77      2176\n",
      "weighted avg       0.79      0.78      0.78      2176\n",
      "\n",
      "\n",
      "Training Set Metrics:\n",
      "Accuracy: 0.8999660902000678\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92      3596\n",
      "           1       0.96      0.77      0.86      2302\n",
      "\n",
      "    accuracy                           0.90      5898\n",
      "   macro avg       0.92      0.88      0.89      5898\n",
      "weighted avg       0.91      0.90      0.90      5898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Creating an XGBoost classifier\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# Training the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "predictions_test = model.predict(X_test)\n",
    "\n",
    "# Calculating accuracy on test set\n",
    "accuracy_test = accuracy_score(y_test, predictions_test)\n",
    "\n",
    "# Making predictions on the training set\n",
    "predictions_train = model.predict(X_train)\n",
    "\n",
    "# Calculating accuracy on training set\n",
    "accuracy_train = accuracy_score(y_train, predictions_train)\n",
    "\n",
    "print(\"Test Set Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions_test))\n",
    "\n",
    "print(\"\\nTraining Set Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_train)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_train, predictions_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rforest:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6196796\ttotal: 1.54ms\tremaining: 1.54ms\n",
      "1:\tlearn: 0.6131165\ttotal: 2.25ms\tremaining: 0us\n",
      "Accuracy: 0.6410845588235294\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.71      0.69      1219\n",
      "           1       0.60      0.55      0.57       957\n",
      "\n",
      "    accuracy                           0.64      2176\n",
      "   macro avg       0.63      0.63      0.63      2176\n",
      "weighted avg       0.64      0.64      0.64      2176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# catboost\n",
    "\n",
    "import catboost as cb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# Initialize CatBoostClassifier\n",
    "\n",
    "model = CatBoostClassifier(iterations=2,\n",
    "                            learning_rate=1,\n",
    "                            depth=2,\n",
    "                            loss_function='Logloss',\n",
    "                            verbose=True)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted classes\n",
    "preds_class = model.predict(X_test)\n",
    "\n",
    "# print results\n",
    "print(\"Accuracy:\", accuracy_score(y_test, preds_class))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, preds_class))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Adaboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "y_pred = adaboost.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
