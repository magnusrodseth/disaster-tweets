{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IT3212 - Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eda = False\n",
    "lemmatize = False\n",
    "with_sentiment = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# NLTK tools and datasets\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Uncomment if you need to download NLTK data packages\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "# Text processing\n",
    "from textblob import TextBlob\n",
    "import contractions\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, classification_report, \n",
    "                             confusion_matrix, roc_curve, auc)\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Miscellaneous\n",
    "from collections import Counter\n",
    "from urllib.parse import unquote\n",
    "from scipy import stats\n",
    "import chardet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix dataset encoding issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some rows in the raw data include non UTF-8 characters. \n",
    "\n",
    "# Example of text with non UTF-8 characters:\n",
    "# 778245336,FALSE,finalized,5,8/30/15 13:27,Not Relevant,0.7952,,army,\n",
    "# text column: Pakistan,\".: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: RT DrAyesha4: #IndiaKoMunTorJawabDo Indian Army ki��_ http://t.co/WJLJq3yA4g\"\n",
    "# ,6.29079E+17,195397186\n",
    "\n",
    "# Chardet identifies the encoding of the raw data as 'MacRoman'.\n",
    "# For now, we will remove all non UTF-8 characters from the raw data\n",
    "# We handle this by removing all � characters from the raw data and writing the modified content back to the file.\n",
    "\n",
    "def fix_non_utf8_encoding(filepath, destination_filepath):\n",
    "    with open(filepath, 'rb') as file:\n",
    "        rawdata = file.read()\n",
    "        result = chardet.detect(rawdata)\n",
    "        print(result['encoding'])\n",
    "\n",
    "\n",
    "    # Open the file in read mode, read its contents, then close it\n",
    "    with open('data/disaster-tweets.csv', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Remove all � characters\n",
    "    content = content.replace('�', '')\n",
    "\n",
    "    # Open the file in write mode and write the modified content back to it\n",
    "    with open(destination_filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "filepath = 'data/disaster-tweets.csv'\n",
    "dest = 'data/disaster-tweets-utf8.csv'\n",
    "\n",
    "# fix_non_utf8_encoding(filepath, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(filepath, destination_filepath_train, destination_filepath_test):\n",
    "    df = pd.read_csv(filepath, encoding='utf-8')\n",
    "    train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "    train_data.to_csv(destination_filepath_train, index=False)\n",
    "    test_data.to_csv(destination_filepath_test, index=False)\n",
    "\n",
    "filepath = 'data/disaster-tweets-utf8.csv'\n",
    "dest_train = 'data/train.csv'\n",
    "dest_test = 'data/test.csv'\n",
    "\n",
    "# split_train_test(filepath, dest_train, dest_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>choose_one:confidence</th>\n",
       "      <th>choose_one_gold</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>778253309</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 16:07</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>screamed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i dont even remember slsp happening i just remember being like wtf and then the lights turned off and everyone screamed for the encore</td>\n",
       "      <td>6.291070e+17</td>\n",
       "      <td>2.327739e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>778251995</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 20:16</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mudslide</td>\n",
       "      <td>Edinburgh</td>\n",
       "      <td>@hazelannmac ooh now I feel guilty about wishing hatman out. I bet the mudslide was delicious!</td>\n",
       "      <td>6.290180e+17</td>\n",
       "      <td>2.750220e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>778247239</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/30/15 0:15</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>collide</td>\n",
       "      <td>planeta H2o</td>\n",
       "      <td>Soultech - Collide (Club Mix) http://t.co/8xIxBsPOT8</td>\n",
       "      <td>6.290920e+17</td>\n",
       "      <td>6.052387e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>778255430</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 17:03</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.7978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wounded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police Officer Wounded Suspect Dead After Exchanging Shots - http://t.co/iPHaZV47g7</td>\n",
       "      <td>6.291190e+17</td>\n",
       "      <td>2.305930e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778255609</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 22:11</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Sunny Southern California</td>\n",
       "      <td>Cramer: Iger's 3 words that wrecked Disney's stock http://t.co/4dGpBAiVL7</td>\n",
       "      <td>6.290800e+17</td>\n",
       "      <td>2.464266e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  778253309    False   finalized                   5     8/27/15 16:07   \n",
       "1  778251995    False   finalized                   5     8/27/15 20:16   \n",
       "2  778247239    False   finalized                   5      8/30/15 0:15   \n",
       "3  778255430    False   finalized                   5     8/27/15 17:03   \n",
       "4  778255609    False   finalized                   5     8/27/15 22:11   \n",
       "\n",
       "     choose_one  choose_one:confidence choose_one_gold   keyword  \\\n",
       "0  Not Relevant                 1.0000             NaN  screamed   \n",
       "1  Not Relevant                 1.0000             NaN  mudslide   \n",
       "2  Not Relevant                 1.0000             NaN   collide   \n",
       "3      Relevant                 0.7978             NaN   wounded   \n",
       "4  Not Relevant                 1.0000             NaN   wrecked   \n",
       "\n",
       "                    location  \\\n",
       "0                        NaN   \n",
       "1                  Edinburgh   \n",
       "2                planeta H2o   \n",
       "3                        NaN   \n",
       "4  Sunny Southern California   \n",
       "\n",
       "                                                                                                                                     text  \\\n",
       "0  i dont even remember slsp happening i just remember being like wtf and then the lights turned off and everyone screamed for the encore   \n",
       "1                                          @hazelannmac ooh now I feel guilty about wishing hatman out. I bet the mudslide was delicious!   \n",
       "2                                                                                    Soultech - Collide (Club Mix) http://t.co/8xIxBsPOT8   \n",
       "3                                                     Police Officer Wounded Suspect Dead After Exchanging Shots - http://t.co/iPHaZV47g7   \n",
       "4                                                               Cramer: Iger's 3 words that wrecked Disney's stock http://t.co/4dGpBAiVL7   \n",
       "\n",
       "        tweetid        userid  \n",
       "0  6.291070e+17  2.327739e+08  \n",
       "1  6.290180e+17  2.750220e+07  \n",
       "2  6.290920e+17  6.052387e+08  \n",
       "3  6.291190e+17  2.305930e+09  \n",
       "4  6.290800e+17  2.464266e+07  "
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import_remote = True\n",
    "\n",
    "if import_remote:\n",
    "    df_train = pd.read_csv('https://raw.githubusercontent.com/magnusrodseth/it3212/main/data/train.csv', encoding='utf-8')\n",
    "    df_test = pd.read_csv('https://raw.githubusercontent.com/magnusrodseth/it3212/main/data/test.csv', encoding='utf-8')\n",
    "else:\n",
    "    df_train = pd.read_csv('./data/train.csv', encoding='utf-8')\n",
    "    df_test = pd.read_csv('./data/test.csv', encoding='utf-8')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    # Clean `keyword` column.\n",
    "\n",
    "    # Write the updated dataframe to a new CSV file\n",
    "    # Plot the most common keywords\n",
    "    defined_keywords = df_train[df_train['keyword'] != '']['keyword']\n",
    "\n",
    "    plt.figure()\n",
    "    sns.countplot(y=defined_keywords, order=defined_keywords.value_counts().iloc[:10].index)\n",
    "    plt.title('Most Common Keywords')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Keyword')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    # Compare keywords for disaster tweets and non-disaster tweets\n",
    "    disaster_keywords = df_train[df_train['choose_one'] == 'Relevant']['keyword']\n",
    "    non_disaster_keywords = df_train[df_train['choose_one'] == 'Not Relevant']['keyword']\n",
    "\n",
    "    # Create a figure object and define the grid\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))  # 1 row, 2 columns\n",
    "\n",
    "    # Plotting\n",
    "    sns.countplot(y=disaster_keywords, ax=ax[0], order=disaster_keywords.value_counts().iloc[:10].index, color='red')\n",
    "    sns.countplot(y=non_disaster_keywords, ax=ax[1], order=non_disaster_keywords.value_counts().iloc[:10].index, color='blue')\n",
    "\n",
    "    # Titles and labels\n",
    "    ax[0].set_title('Most Common Keywords for Disaster Tweets')\n",
    "    ax[0].set_xlabel('Count')\n",
    "    ax[0].set_ylabel('Keyword')\n",
    "\n",
    "    ax[1].set_title('Most Common Keywords for Non-Disaster Tweets')\n",
    "    ax[1].set_xlabel('Count')\n",
    "    ax[1].set_ylabel('Keyword')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, we can see that the top 10 shared keywords of disaster-related tweets and non-disaster-related tweets do not share any common keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 2167 of total: 8700 rows. Remaining rows: 6533\n",
      "Removed 635 duplicated rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sunset looked like erupting volcano initial thought pixar short lava</td>\n",
       "      <td>volcano</td>\n",
       "      <td>The sunset looked like an erupting volcano .... My initial thought was the Pixar short Lava http://t.co/g4sChqFEsT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7294 nikon d50 61 mp digital slr camera body 2 batteries carry bag charger 20000</td>\n",
       "      <td>body bag</td>\n",
       "      <td>#7294 Nikon D50 6.1 MP Digital SLR Camera Body 2 batteries carry bag and charger http://t.co/SL7PHqSGKV\\n\\n$200.00\\n_ http://t.co/T4Qh2OM8Op</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>mentaltwitter note make sure smoke alarm battery snuff times face many twitter reminders changing battery</td>\n",
       "      <td>smoke</td>\n",
       "      <td>Mental/Twitter Note: Make sure my smoke alarm battery is up to snuff at all times or face many twitter reminders of changing my battery.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>emergency need part 2 3 nashnewvideo nashgrier 103</td>\n",
       "      <td>emergency</td>\n",
       "      <td>?????? EMERGENCY ?????? NEED PART 2 and 3!!! #NashNewVideo http://t.co/TwdnNaIOns @Nashgrier 103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>whelen model 295ss100 siren amplifier police emergency vehicle full read ebay</td>\n",
       "      <td>siren</td>\n",
       "      <td>WHELEN MODEL 295SS-100 SIREN AMPLIFIER POLICE EMERGENCY VEHICLE - Full read by eBay http://t.co/Q3yYQi4A27 http://t.co/whEreofYAx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  \\\n",
       "0       1   \n",
       "1       1   \n",
       "2       0   \n",
       "3       0   \n",
       "4       0   \n",
       "\n",
       "                                                                                                        text  \\\n",
       "0                                       sunset looked like erupting volcano initial thought pixar short lava   \n",
       "1                           7294 nikon d50 61 mp digital slr camera body 2 batteries carry bag charger 20000   \n",
       "2  mentaltwitter note make sure smoke alarm battery snuff times face many twitter reminders changing battery   \n",
       "3                                                         emergency need part 2 3 nashnewvideo nashgrier 103   \n",
       "4                              whelen model 295ss100 siren amplifier police emergency vehicle full read ebay   \n",
       "\n",
       "     keyword  \\\n",
       "0    volcano   \n",
       "1   body bag   \n",
       "2      smoke   \n",
       "3  emergency   \n",
       "4      siren   \n",
       "\n",
       "                                                                                                                                       text_raw  \n",
       "0                            The sunset looked like an erupting volcano .... My initial thought was the Pixar short Lava http://t.co/g4sChqFEsT  \n",
       "1  #7294 Nikon D50 6.1 MP Digital SLR Camera Body 2 batteries carry bag and charger http://t.co/SL7PHqSGKV\\n\\n$200.00\\n_ http://t.co/T4Qh2OM8Op  \n",
       "2      Mental/Twitter Note: Make sure my smoke alarm battery is up to snuff at all times or face many twitter reminders of changing my battery.  \n",
       "3                                              ?????? EMERGENCY ?????? NEED PART 2 and 3!!! #NashNewVideo http://t.co/TwdnNaIOns @Nashgrier 103  \n",
       "4             WHELEN MODEL 295SS-100 SIREN AMPLIFIER POLICE EMERGENCY VEHICLE - Full read by eBay http://t.co/Q3yYQi4A27 http://t.co/whEreofYAx  "
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def filter_rows_by_confidence_and_decision(df, confidence_threshold):\n",
    "    df = df[df['choose_one:confidence'] >= confidence_threshold]\n",
    "    df = df[df['choose_one'] != \"Can't Decide\"]\n",
    "    return df\n",
    "\n",
    "def map_choose_one_to_y(df):\n",
    "    df['target'] = df['choose_one'].apply(lambda choice: 1 if choice == 'Relevant' else 0)\n",
    "    return df\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words(\"english\")])\n",
    "    if lemmatize:\n",
    "        text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    text = contractions.fix(text)\n",
    "    text = ' '.join(tokenizer.tokenize(text))\n",
    "    return text\n",
    "\n",
    "def clean_keyword(keyword):\n",
    "    return unquote(keyword) if pd.notnull(keyword) else ''\n",
    "\n",
    "def clean_data(df):\n",
    "    df['keyword'] = df['keyword'].apply(clean_keyword).apply(str.lower)\n",
    "    df['text_raw'] = df['text']\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "initial_count = df_train.shape[0]\n",
    "confidence_threshold = 0.7\n",
    "\n",
    "df_train = filter_rows_by_confidence_and_decision(df_train, confidence_threshold)\n",
    "print(\"Removed {} of total: {} rows. Remaining rows: {}\".format(initial_count - df_train.shape[0], initial_count, df_train.shape[0]))\n",
    "\n",
    "features_to_keep = ['target', 'text', 'keyword']\n",
    "\n",
    "df_train = map_choose_one_to_y(df_train)\n",
    "df_train = df_train[features_to_keep]\n",
    "df_train = clean_data(df_train)\n",
    "\n",
    "count_initial = df_train.shape[0]\n",
    "df_train = df_train.drop_duplicates(subset=['text'])\n",
    "print(\"Removed {} duplicated rows.\".format(count_initial - df_train.shape[0]))\n",
    "\n",
    "\n",
    "# Preprocess the test data as well\n",
    "df_test = map_choose_one_to_y(df_test)\n",
    "df_test = df_test[features_to_keep]\n",
    "df_test = clean_data(df_test)\n",
    "\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features that can be extracted from the raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text_raw</th>\n",
       "      <th>text_length</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>has_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>do not even remember slsp happening remember like wtf lights turned everyone screamed encore</td>\n",
       "      <td>screamed</td>\n",
       "      <td>i dont even remember slsp happening i just remember being like wtf and then the lights turned off and everyone screamed for the encore</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>hazelannmac ooh feel guilty wishing hatman bet mudslide delicious</td>\n",
       "      <td>mudslide</td>\n",
       "      <td>@hazelannmac ooh now I feel guilty about wishing hatman out. I bet the mudslide was delicious!</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>soultech collide club mix</td>\n",
       "      <td>collide</td>\n",
       "      <td>Soultech - Collide (Club Mix) http://t.co/8xIxBsPOT8</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>police officer wounded suspect dead exchanging shots</td>\n",
       "      <td>wounded</td>\n",
       "      <td>Police Officer Wounded Suspect Dead After Exchanging Shots - http://t.co/iPHaZV47g7</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>cramer igers 3 words wrecked disneys stock</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Cramer: Iger's 3 words that wrecked Disney's stock http://t.co/4dGpBAiVL7</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  \\\n",
       "0       0   \n",
       "1       0   \n",
       "2       0   \n",
       "3       1   \n",
       "4       0   \n",
       "\n",
       "                                                                                           text  \\\n",
       "0  do not even remember slsp happening remember like wtf lights turned everyone screamed encore   \n",
       "1                             hazelannmac ooh feel guilty wishing hatman bet mudslide delicious   \n",
       "2                                                                     soultech collide club mix   \n",
       "3                                          police officer wounded suspect dead exchanging shots   \n",
       "4                                                    cramer igers 3 words wrecked disneys stock   \n",
       "\n",
       "    keyword  \\\n",
       "0  screamed   \n",
       "1  mudslide   \n",
       "2   collide   \n",
       "3   wounded   \n",
       "4   wrecked   \n",
       "\n",
       "                                                                                                                                 text_raw  \\\n",
       "0  i dont even remember slsp happening i just remember being like wtf and then the lights turned off and everyone screamed for the encore   \n",
       "1                                          @hazelannmac ooh now I feel guilty about wishing hatman out. I bet the mudslide was delicious!   \n",
       "2                                                                                    Soultech - Collide (Club Mix) http://t.co/8xIxBsPOT8   \n",
       "3                                                     Police Officer Wounded Suspect Dead After Exchanging Shots - http://t.co/iPHaZV47g7   \n",
       "4                                                               Cramer: Iger's 3 words that wrecked Disney's stock http://t.co/4dGpBAiVL7   \n",
       "\n",
       "   text_length  hashtag_count  mention_count  has_url  \n",
       "0          134              0              0        0  \n",
       "1           94              0              1        0  \n",
       "2           52              0              0        1  \n",
       "3           83              0              0        1  \n",
       "4           73              0              0        1  "
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features(df): \n",
    "    # Create new column for text length\n",
    "    df['text_length'] = df['text_raw'].apply(len)\n",
    "    # Extract the number of hashtags\n",
    "    df[\"hashtag_count\"] = df[\"text_raw\"].apply(lambda x: len([c for c in str(x) if c == \"#\"]))\n",
    "\n",
    "    # Extract the number of mentions\n",
    "    df[\"mention_count\"] = df[\"text_raw\"].apply(lambda x: len([c for c in str(x) if c == \"@\"]))\n",
    "\n",
    "    # Extract the `has_url` feature\n",
    "    df[\"has_url\"] = df[\"text_raw\"].apply(lambda x: 1 if \"http\" in str(x) else 0)\n",
    "    return df\n",
    "\n",
    "# Write the updated dataframe to a CSV file\n",
    "df_train = extract_features(df_train)\n",
    "df_test = extract_features(df_test)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(text, n):\n",
    "    tokens = word_tokenize(text)\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return n_grams\n",
    "\n",
    "\n",
    "df_train['bigrams'] = df_train['text'].apply(lambda x: create_ngrams(x, 2))\n",
    "df_train['trigrams'] = df_train['text'].apply(lambda x: create_ngrams(x, 3))\n",
    "\n",
    "df_test['bigrams'] = df_test['text'].apply(lambda x: create_ngrams(x, 2))\n",
    "df_test['trigrams'] = df_test['text'].apply(lambda x: create_ngrams(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams_string(ngram_list):\n",
    "    ngram_words = ['_'.join(ngram) for ngram in ngram_list]\n",
    "    ngram_string = ' '.join(ngram_words)\n",
    "    return ngram_string\n",
    "\n",
    "def add_ngrams(df):\n",
    "    df['bigrams'] = df['bigrams'].apply(lambda x: create_ngrams_string(x))\n",
    "    df['trigrams'] = df['trigrams'].apply(lambda x: create_ngrams_string(x))\n",
    "\n",
    "    df['text_with_ngrams'] = df['text'] + ' ' +  df['bigrams'] + ' ' + df['trigrams'] \n",
    "    return df\n",
    "\n",
    "df_train = add_ngrams(df_train)\n",
    "df_test = add_ngrams(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1313843, 3061160)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "\n",
    "tokenized_text = df_train['text_with_ngrams'].apply(lambda x: x.split())\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_text,\n",
    "            vector_size=400, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2, # Ignores all words with total frequency lower than 2.                                  \n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 32, # no.of cores\n",
    "            seed = 34\n",
    ") \n",
    "\n",
    "model_w2v.train(tokenized_text, total_examples= len(df_train['text']), epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that converts tokens to vectors using the Word2Vec model\n",
    "def tokens_to_vectors(tokens, model, vector_size):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens to their corresponding vectors using a Word2Vec model.\n",
    "\n",
    "    Args:\n",
    "    - tokens (list of str): A list of tokens (words).\n",
    "    - model (gensim.models.Word2Vec): The trained Word2Vec model.\n",
    "    - vector_size (int): The size of the vectors.\n",
    "\n",
    "    Returns:\n",
    "    - list of np.ndarray: A list of vectors corresponding to the tokens.\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((len(tokens), vector_size))\n",
    "    for i, token in enumerate(tokens):\n",
    "        try:\n",
    "            vectors[i] = model.wv[token]\n",
    "        except KeyError:  # Token not in the model's vocabulary\n",
    "            vectors[i] = np.zeros(vector_size)\n",
    "    return vectors.mean(axis=0)\n",
    "\n",
    "# Example usage\n",
    "vecs_train = [tokens_to_vectors(tokens, model_w2v, 400) for tokens in tokenized_text]\n",
    "vecs_train = np.vstack(vecs_train)\n",
    "\n",
    "vecs_test = [tokens_to_vectors(tokens, model_w2v, 400) for tokens in df_test['text'].apply(lambda x: x.split())]\n",
    "vecs_test = np.vstack(vecs_test)\n",
    "# # Converting the list of vectors to a DataFrame\n",
    "# vectors_df = pd.DataFrame(np.vstack(vecs))\n",
    "# print(vectors_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedded_w2v_df = pd.DataFrame(vecs_train, columns=[str(x) for x in range(400)], index=df_train.index) \n",
    "text_embedded_w2f_df_test = pd.DataFrame(vecs_test, columns=[str(x) for x in range(400)], index=df_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing this for later we can test different features without having to re-run cells above this one\n",
    "df_checkpoint = df_train.copy(deep=True)\n",
    "df_test_checkpoint = df_test.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['target', 'text', 'keyword', 'text_raw', 'text_length', 'hashtag_count',\n",
       "       'mention_count', 'has_url', 'bigrams', 'trigrams', 'text_with_ngrams'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = ['target', 'text_length', 'hashtag_count', 'mention_count', 'has_url']\n",
    "\n",
    "df_checkpoint = df_train[features_to_keep]\n",
    "df_test_checkpoint = df_test[features_to_keep]\n",
    "\n",
    "\n",
    "# Concatenate the dataframes with td-idf features for the text feature\n",
    "# df_checkpoint = pd.concat([df_checkpoint, text_embedded_df, keyword_embedded_df], axis=1)\n",
    "# df_test_checkpoint = pd.concat([df_test_checkpoint, text_embedded_test_df, keyword_embedded_test_df], axis=1)\n",
    "\n",
    "df_checkpoint = pd.concat([df_checkpoint, text_embedded_w2v_df], axis=1)\n",
    "df_test_checkpoint = pd.concat([df_test_checkpoint, text_embedded_w2f_df_test], axis=1)\n",
    "\n",
    "df_checkpoint.dropna(inplace=True)\n",
    "df_test_checkpoint.dropna(inplace=True)\n",
    "\n",
    "# extract y_train and y_test here to avoid column name collision with 'target' feature coming from text and keyword embeddings\n",
    "y_train = df_checkpoint['target']\n",
    "y_test = df_test_checkpoint['target']\n",
    "\n",
    "X_train = df_checkpoint.drop(['target'], axis=1)\n",
    "X_test = df_test_checkpoint.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(y_pred, y_train, y_pred_test, y_test):\n",
    "    print(\"Train results\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Train accuracy: {}\".format(accuracy_score(y_train, y_pred)))\n",
    "    print(classification_report(y_train, y_pred))\n",
    "    print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "    print()\n",
    "    print(\"Test results\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Test accuracy: {}\".format(accuracy_score(y_test, y_pred_test)))\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results\n",
      "-----------------------------\n",
      "Train accuracy: 0.8414448024419197\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88      3595\n",
      "           1       0.85      0.72      0.78      2302\n",
      "\n",
      "    accuracy                           0.84      5897\n",
      "   macro avg       0.84      0.82      0.83      5897\n",
      "weighted avg       0.84      0.84      0.84      5897\n",
      "\n",
      "[[3315  280]\n",
      " [ 655 1647]]\n",
      "\n",
      "Test results\n",
      "-----------------------------\n",
      "Test accuracy: 0.7591911764705882\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.75      0.78      1219\n",
      "           1       0.71      0.78      0.74       957\n",
      "\n",
      "    accuracy                           0.76      2176\n",
      "   macro avg       0.76      0.76      0.76      2176\n",
      "weighted avg       0.76      0.76      0.76      2176\n",
      "\n",
      "[[910 309]\n",
      " [215 742]]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(random_state=42, solver=\"liblinear\")\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = cross_val_predict(logreg, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "y_pred_test = logreg.predict(X_test)\n",
    "\n",
    "print_results(y_pred, y_train, y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results\n",
      "-----------------------------\n",
      "Train accuracy: 0.8414448024419197\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88      3595\n",
      "           1       0.86      0.70      0.78      2302\n",
      "\n",
      "    accuracy                           0.84      5897\n",
      "   macro avg       0.85      0.82      0.83      5897\n",
      "weighted avg       0.84      0.84      0.84      5897\n",
      "\n",
      "[[3342  253]\n",
      " [ 682 1620]]\n",
      "\n",
      "Test results\n",
      "-----------------------------\n",
      "Test accuracy: 0.7545955882352942\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.74      0.77      1219\n",
      "           1       0.70      0.78      0.74       957\n",
      "\n",
      "    accuracy                           0.75      2176\n",
      "   macro avg       0.75      0.76      0.75      2176\n",
      "weighted avg       0.76      0.75      0.76      2176\n",
      "\n",
      "[[899 320]\n",
      " [214 743]]\n"
     ]
    }
   ],
   "source": [
    "first_n = 1000\n",
    "\n",
    "# Initialize SVM model\n",
    "svm_model = SVC(kernel='linear', C=1, random_state=42, probability=False)\n",
    "\n",
    "# Fit the model on training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Use 5-fold cross-validation to get predictions on training set\n",
    "y_pred_train = cross_val_predict(svm_model, X_train, y_train, cv=5)\n",
    "y_pred_test = svm_model.predict(X_test)\n",
    "\n",
    "print_results(y_pred_train, y_train, y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7637867647058824\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.79      1219\n",
      "           1       0.75      0.70      0.72       957\n",
      "\n",
      "    accuracy                           0.76      2176\n",
      "   macro avg       0.76      0.76      0.76      2176\n",
      "weighted avg       0.76      0.76      0.76      2176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#Creating an XGBoost classifier\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "#Training the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Making predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "#Calculating accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7449448529411765\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.82      0.78      1219\n",
      "           1       0.74      0.64      0.69       957\n",
      "\n",
      "    accuracy                           0.74      2176\n",
      "   macro avg       0.74      0.73      0.74      2176\n",
      "weighted avg       0.74      0.74      0.74      2176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
