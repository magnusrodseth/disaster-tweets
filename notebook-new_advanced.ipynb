{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IT3212 - Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eda = False\n",
    "lemmatize = False\n",
    "with_sentiment = False\n",
    "with_svm = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# NLTK tools and datasets\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Uncomment if you need to download NLTK data packages\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "# Text processing\n",
    "from textblob import TextBlob\n",
    "import contractions\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, classification_report, \n",
    "                             confusion_matrix, roc_curve, auc)\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Miscellaneous\n",
    "from collections import Counter\n",
    "from urllib.parse import unquote\n",
    "from scipy import stats\n",
    "import chardet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix dataset encoding issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some rows in the raw data include non UTF-8 characters. \n",
    "\n",
    "# Example of text with non UTF-8 characters:\n",
    "# 778245336,FALSE,finalized,5,8/30/15 13:27,Not Relevant,0.7952,,army,\n",
    "# text column: Pakistan,\".: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: .: RT DrAyesha4: #IndiaKoMunTorJawabDo Indian Army ki��_ http://t.co/WJLJq3yA4g\"\n",
    "# ,6.29079E+17,195397186\n",
    "\n",
    "# Chardet identifies the encoding of the raw data as 'MacRoman'.\n",
    "# For now, we will remove all non UTF-8 characters from the raw data\n",
    "# We handle this by removing all � characters from the raw data and writing the modified content back to the file.\n",
    "\n",
    "def fix_non_utf8_encoding(filepath, destination_filepath):\n",
    "    with open(filepath, 'rb') as file:\n",
    "        rawdata = file.read()\n",
    "        result = chardet.detect(rawdata)\n",
    "        print(result['encoding'])\n",
    "\n",
    "\n",
    "    # Open the file in read mode, read its contents, then close it\n",
    "    with open('data/disaster-tweets.csv', 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Remove all � characters\n",
    "    content = content.replace('�', '')\n",
    "\n",
    "    # Open the file in write mode and write the modified content back to it\n",
    "    with open(destination_filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "filepath = 'data/disaster-tweets.csv'\n",
    "dest = 'data/disaster-tweets-utf8.csv'\n",
    "\n",
    "# fix_non_utf8_encoding(filepath, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(filepath, destination_filepath_train, destination_filepath_test):\n",
    "    df = pd.read_csv(filepath, encoding='utf-8')\n",
    "    train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "    train_data.to_csv(destination_filepath_train, index=False)\n",
    "    test_data.to_csv(destination_filepath_test, index=False)\n",
    "\n",
    "filepath = 'data/disaster-tweets-utf8.csv'\n",
    "dest_train = 'data/train.csv'\n",
    "dest_test = 'data/test.csv'\n",
    "\n",
    "# split_train_test(filepath, dest_train, dest_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>choose_one:confidence</th>\n",
       "      <th>choose_one_gold</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>778253309</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 16:07</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>screamed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i dont even remember slsp happening i just rem...</td>\n",
       "      <td>6.291070e+17</td>\n",
       "      <td>2.327739e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>778251995</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 20:16</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mudslide</td>\n",
       "      <td>Edinburgh</td>\n",
       "      <td>@hazelannmac ooh now I feel guilty about wishi...</td>\n",
       "      <td>6.290180e+17</td>\n",
       "      <td>2.750220e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>778247239</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/30/15 0:15</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>collide</td>\n",
       "      <td>planeta H2o</td>\n",
       "      <td>Soultech - Collide (Club Mix) http://t.co/8xIx...</td>\n",
       "      <td>6.290920e+17</td>\n",
       "      <td>6.052387e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>778255430</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 17:03</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.7978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wounded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police Officer Wounded Suspect Dead After Exch...</td>\n",
       "      <td>6.291190e+17</td>\n",
       "      <td>2.305930e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778255609</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>8/27/15 22:11</td>\n",
       "      <td>Not Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Sunny Southern California</td>\n",
       "      <td>Cramer: Iger's 3 words that wrecked Disney's s...</td>\n",
       "      <td>6.290800e+17</td>\n",
       "      <td>2.464266e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  778253309    False   finalized                   5     8/27/15 16:07   \n",
       "1  778251995    False   finalized                   5     8/27/15 20:16   \n",
       "2  778247239    False   finalized                   5      8/30/15 0:15   \n",
       "3  778255430    False   finalized                   5     8/27/15 17:03   \n",
       "4  778255609    False   finalized                   5     8/27/15 22:11   \n",
       "\n",
       "     choose_one  choose_one:confidence choose_one_gold   keyword  \\\n",
       "0  Not Relevant                 1.0000             NaN  screamed   \n",
       "1  Not Relevant                 1.0000             NaN  mudslide   \n",
       "2  Not Relevant                 1.0000             NaN   collide   \n",
       "3      Relevant                 0.7978             NaN   wounded   \n",
       "4  Not Relevant                 1.0000             NaN   wrecked   \n",
       "\n",
       "                    location  \\\n",
       "0                        NaN   \n",
       "1                  Edinburgh   \n",
       "2                planeta H2o   \n",
       "3                        NaN   \n",
       "4  Sunny Southern California   \n",
       "\n",
       "                                                text       tweetid  \\\n",
       "0  i dont even remember slsp happening i just rem...  6.291070e+17   \n",
       "1  @hazelannmac ooh now I feel guilty about wishi...  6.290180e+17   \n",
       "2  Soultech - Collide (Club Mix) http://t.co/8xIx...  6.290920e+17   \n",
       "3  Police Officer Wounded Suspect Dead After Exch...  6.291190e+17   \n",
       "4  Cramer: Iger's 3 words that wrecked Disney's s...  6.290800e+17   \n",
       "\n",
       "         userid  \n",
       "0  2.327739e+08  \n",
       "1  2.750220e+07  \n",
       "2  6.052387e+08  \n",
       "3  2.305930e+09  \n",
       "4  2.464266e+07  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import_remote = True\n",
    "\n",
    "if import_remote:\n",
    "    df_train = pd.read_csv('https://raw.githubusercontent.com/magnusrodseth/it3212/main/data/train.csv', encoding='utf-8')\n",
    "    df_test = pd.read_csv('https://raw.githubusercontent.com/magnusrodseth/it3212/main/data/test.csv', encoding='utf-8')\n",
    "else:\n",
    "    df_train = pd.read_csv('./data/train.csv', encoding='utf-8')\n",
    "    df_test = pd.read_csv('./data/test.csv', encoding='utf-8')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    # Clean `keyword` column.\n",
    "\n",
    "    # Write the updated dataframe to a new CSV file\n",
    "    # Plot the most common keywords\n",
    "    defined_keywords = df_train[df_train['keyword'] != '']['keyword']\n",
    "\n",
    "    plt.figure()\n",
    "    sns.countplot(y=defined_keywords, order=defined_keywords.value_counts().iloc[:10].index)\n",
    "    plt.title('Most Common Keywords')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Keyword')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_eda:\n",
    "    # Compare keywords for disaster tweets and non-disaster tweets\n",
    "    disaster_keywords = df_train[df_train['choose_one'] == 'Relevant']['keyword']\n",
    "    non_disaster_keywords = df_train[df_train['choose_one'] == 'Not Relevant']['keyword']\n",
    "\n",
    "    # Create a figure object and define the grid\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))  # 1 row, 2 columns\n",
    "\n",
    "    # Plotting\n",
    "    sns.countplot(y=disaster_keywords, ax=ax[0], order=disaster_keywords.value_counts().iloc[:10].index, color='red')\n",
    "    sns.countplot(y=non_disaster_keywords, ax=ax[1], order=non_disaster_keywords.value_counts().iloc[:10].index, color='blue')\n",
    "\n",
    "    # Titles and labels\n",
    "    ax[0].set_title('Most Common Keywords for Disaster Tweets')\n",
    "    ax[0].set_xlabel('Count')\n",
    "    ax[0].set_ylabel('Keyword')\n",
    "\n",
    "    ax[1].set_title('Most Common Keywords for Non-Disaster Tweets')\n",
    "    ax[1].set_xlabel('Count')\n",
    "    ax[1].set_ylabel('Keyword')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, we can see that the top 10 shared keywords of disaster-related tweets and non-disaster-related tweets do not share any common keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 2167 of total: 8700 rows. Remaining rows: 6533\n",
      "Removed 635 duplicated rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sunset looked like erupting volcano initial th...</td>\n",
       "      <td>volcano</td>\n",
       "      <td>The sunset looked like an erupting volcano ......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7294 nikon d50 61 mp digital slr camera body 2...</td>\n",
       "      <td>body bag</td>\n",
       "      <td>#7294 Nikon D50 6.1 MP Digital SLR Camera Body...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>mentaltwitter note make sure smoke alarm batte...</td>\n",
       "      <td>smoke</td>\n",
       "      <td>Mental/Twitter Note: Make sure my smoke alarm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>emergency need part 2 3 nashnewvideo nashgrier...</td>\n",
       "      <td>emergency</td>\n",
       "      <td>?????? EMERGENCY ?????? NEED PART 2 and 3!!! #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>whelen model 295ss100 siren amplifier police e...</td>\n",
       "      <td>siren</td>\n",
       "      <td>WHELEN MODEL 295SS-100 SIREN AMPLIFIER POLICE ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text    keyword  \\\n",
       "0       1  sunset looked like erupting volcano initial th...    volcano   \n",
       "1       1  7294 nikon d50 61 mp digital slr camera body 2...   body bag   \n",
       "2       0  mentaltwitter note make sure smoke alarm batte...      smoke   \n",
       "3       0  emergency need part 2 3 nashnewvideo nashgrier...  emergency   \n",
       "4       0  whelen model 295ss100 siren amplifier police e...      siren   \n",
       "\n",
       "                                            text_raw  \n",
       "0  The sunset looked like an erupting volcano ......  \n",
       "1  #7294 Nikon D50 6.1 MP Digital SLR Camera Body...  \n",
       "2  Mental/Twitter Note: Make sure my smoke alarm ...  \n",
       "3  ?????? EMERGENCY ?????? NEED PART 2 and 3!!! #...  \n",
       "4  WHELEN MODEL 295SS-100 SIREN AMPLIFIER POLICE ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def filter_rows_by_confidence_and_decision(df, confidence_threshold):\n",
    "    df = df[df['choose_one:confidence'] >= confidence_threshold]\n",
    "    df = df[df['choose_one'] != \"Can't Decide\"]\n",
    "    return df\n",
    "\n",
    "def map_choose_one_to_y(df):\n",
    "    df['target'] = df['choose_one'].apply(lambda choice: 1 if choice == 'Relevant' else 0)\n",
    "    return df\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words(\"english\")])\n",
    "    if lemmatize:\n",
    "        text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    text = contractions.fix(text)\n",
    "    text = ' '.join(tokenizer.tokenize(text))\n",
    "    return text\n",
    "\n",
    "def clean_keyword(keyword):\n",
    "    return unquote(keyword) if pd.notnull(keyword) else ''\n",
    "\n",
    "def clean_data(df):\n",
    "    df['keyword'] = df['keyword'].apply(clean_keyword).apply(str.lower)\n",
    "    df['text_raw'] = df['text']\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "initial_count = df_train.shape[0]\n",
    "confidence_threshold = 0.7\n",
    "\n",
    "df_train = filter_rows_by_confidence_and_decision(df_train, confidence_threshold)\n",
    "print(\"Removed {} of total: {} rows. Remaining rows: {}\".format(initial_count - df_train.shape[0], initial_count, df_train.shape[0]))\n",
    "\n",
    "features_to_keep = ['target', 'text', 'keyword']\n",
    "\n",
    "df_train = map_choose_one_to_y(df_train)\n",
    "df_train = df_train[features_to_keep]\n",
    "df_train = clean_data(df_train)\n",
    "\n",
    "count_initial = df_train.shape[0]\n",
    "df_train = df_train.drop_duplicates(subset=['text'])\n",
    "print(\"Removed {} duplicated rows.\".format(count_initial - df_train.shape[0]))\n",
    "\n",
    "\n",
    "# Preprocess the test data as well\n",
    "df_test = map_choose_one_to_y(df_test)\n",
    "df_test = df_test[features_to_keep]\n",
    "df_test = clean_data(df_test)\n",
    "\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features that can be extracted from the raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text_raw</th>\n",
       "      <th>text_length</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>has_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>do not even remember slsp happening remember l...</td>\n",
       "      <td>screamed</td>\n",
       "      <td>i dont even remember slsp happening i just rem...</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>hazelannmac ooh feel guilty wishing hatman bet...</td>\n",
       "      <td>mudslide</td>\n",
       "      <td>@hazelannmac ooh now I feel guilty about wishi...</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>soultech collide club mix</td>\n",
       "      <td>collide</td>\n",
       "      <td>Soultech - Collide (Club Mix) http://t.co/8xIx...</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>police officer wounded suspect dead exchanging...</td>\n",
       "      <td>wounded</td>\n",
       "      <td>Police Officer Wounded Suspect Dead After Exch...</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>cramer igers 3 words wrecked disneys stock</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Cramer: Iger's 3 words that wrecked Disney's s...</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text   keyword  \\\n",
       "0       0  do not even remember slsp happening remember l...  screamed   \n",
       "1       0  hazelannmac ooh feel guilty wishing hatman bet...  mudslide   \n",
       "2       0                          soultech collide club mix   collide   \n",
       "3       1  police officer wounded suspect dead exchanging...   wounded   \n",
       "4       0         cramer igers 3 words wrecked disneys stock   wrecked   \n",
       "\n",
       "                                            text_raw  text_length  \\\n",
       "0  i dont even remember slsp happening i just rem...          134   \n",
       "1  @hazelannmac ooh now I feel guilty about wishi...           94   \n",
       "2  Soultech - Collide (Club Mix) http://t.co/8xIx...           52   \n",
       "3  Police Officer Wounded Suspect Dead After Exch...           83   \n",
       "4  Cramer: Iger's 3 words that wrecked Disney's s...           73   \n",
       "\n",
       "   hashtag_count  mention_count  has_url  \n",
       "0              0              0        0  \n",
       "1              0              1        0  \n",
       "2              0              0        1  \n",
       "3              0              0        1  \n",
       "4              0              0        1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features(df): \n",
    "    # Create new column for text length\n",
    "    df['text_length'] = df['text_raw'].apply(len)\n",
    "    # Extract the number of hashtags\n",
    "    df[\"hashtag_count\"] = df[\"text_raw\"].apply(lambda x: len([c for c in str(x) if c == \"#\"]))\n",
    "\n",
    "    # Extract the number of mentions\n",
    "    df[\"mention_count\"] = df[\"text_raw\"].apply(lambda x: len([c for c in str(x) if c == \"@\"]))\n",
    "\n",
    "    # Extract the `has_url` feature\n",
    "    df[\"has_url\"] = df[\"text_raw\"].apply(lambda x: 1 if \"http\" in str(x) else 0)\n",
    "    return df\n",
    "\n",
    "# Write the updated dataframe to a CSV file\n",
    "df_train = extract_features(df_train)\n",
    "df_test = extract_features(df_test)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(text, n):\n",
    "    tokens = word_tokenize(text)\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return n_grams\n",
    "\n",
    "\n",
    "df_train['bigrams'] = df_train['text'].apply(lambda x: create_ngrams(x, 2))\n",
    "df_train['trigrams'] = df_train['text'].apply(lambda x: create_ngrams(x, 3))\n",
    "\n",
    "df_test['bigrams'] = df_test['text'].apply(lambda x: create_ngrams(x, 2))\n",
    "df_test['trigrams'] = df_test['text'].apply(lambda x: create_ngrams(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams_string(ngram_list):\n",
    "    ngram_words = ['_'.join(ngram) for ngram in ngram_list]\n",
    "    ngram_string = ' '.join(ngram_words)\n",
    "    return ngram_string\n",
    "\n",
    "def add_ngrams(df):\n",
    "    df['bigrams'] = df['bigrams'].apply(lambda x: create_ngrams_string(x))\n",
    "    df['trigrams'] = df['trigrams'].apply(lambda x: create_ngrams_string(x))\n",
    "\n",
    "    df['text_with_ngrams'] = df['text'] + ' ' +  df['bigrams'] + ' ' + df['trigrams'] \n",
    "    return df\n",
    "\n",
    "df_train = add_ngrams(df_train)\n",
    "df_test = add_ngrams(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1313989, 3061160)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "\n",
    "tokenized_text = df_train['text_with_ngrams'].apply(lambda x: x.split())\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_text,\n",
    "            vector_size=400, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2, # Ignores all words with total frequency lower than 2.                                  \n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 32, # no.of cores\n",
    "            seed = 34\n",
    ") \n",
    "\n",
    "model_w2v.train(tokenized_text, total_examples= len(df_train['text']), epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6f/f4jmdnpx54s3hmfy9g0jlvch0000gn/T/ipykernel_14292/2475012950.py:20: RuntimeWarning: Mean of empty slice.\n",
      "  return vectors.mean(axis=0)\n",
      "/Users/magnusrodseth/anaconda3/lib/python3.11/site-packages/numpy/core/_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    }
   ],
   "source": [
    "# Define a function that converts tokens to vectors using the Word2Vec model\n",
    "def tokens_to_vectors(tokens, model, vector_size):\n",
    "    \"\"\"\n",
    "    Convert a list of tokens to their corresponding vectors using a Word2Vec model.\n",
    "\n",
    "    Args:\n",
    "    - tokens (list of str): A list of tokens (words).\n",
    "    - model (gensim.models.Word2Vec): The trained Word2Vec model.\n",
    "    - vector_size (int): The size of the vectors.\n",
    "\n",
    "    Returns:\n",
    "    - list of np.ndarray: A list of vectors corresponding to the tokens.\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((len(tokens), vector_size))\n",
    "    for i, token in enumerate(tokens):\n",
    "        try:\n",
    "            vectors[i] = model.wv[token]\n",
    "        except KeyError:  # Token not in the model's vocabulary\n",
    "            vectors[i] = np.zeros(vector_size)\n",
    "    return vectors.mean(axis=0)\n",
    "\n",
    "# Example usage\n",
    "vecs_train = [tokens_to_vectors(tokens, model_w2v, 400) for tokens in tokenized_text]\n",
    "vecs_train = np.vstack(vecs_train)\n",
    "\n",
    "vecs_test = [tokens_to_vectors(tokens, model_w2v, 400) for tokens in df_test['text'].apply(lambda x: x.split())]\n",
    "vecs_test = np.vstack(vecs_test)\n",
    "# # Converting the list of vectors to a DataFrame\n",
    "# vectors_df = pd.DataFrame(np.vstack(vecs))\n",
    "# print(vectors_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedded_w2v_df = pd.DataFrame(vecs_train, columns=[str(x) for x in range(400)], index=df_train.index) \n",
    "text_embedded_w2f_df_test = pd.DataFrame(vecs_test, columns=[str(x) for x in range(400)], index=df_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing this for later we can test different features without having to re-run cells above this one\n",
    "df_checkpoint = df_train.copy(deep=True)\n",
    "df_test_checkpoint = df_test.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['target', 'text', 'keyword', 'text_raw', 'text_length', 'hashtag_count',\n",
       "       'mention_count', 'has_url', 'bigrams', 'trigrams', 'text_with_ngrams'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = ['target', 'text_length', 'hashtag_count', 'mention_count', 'has_url']\n",
    "\n",
    "df_checkpoint = df_train[features_to_keep]\n",
    "df_test_checkpoint = df_test[features_to_keep]\n",
    "\n",
    "\n",
    "# Concatenate the dataframes with td-idf features for the text feature\n",
    "# df_checkpoint = pd.concat([df_checkpoint, text_embedded_df, keyword_embedded_df], axis=1)\n",
    "# df_test_checkpoint = pd.concat([df_test_checkpoint, text_embedded_test_df, keyword_embedded_test_df], axis=1)\n",
    "\n",
    "df_checkpoint = pd.concat([df_checkpoint, text_embedded_w2v_df], axis=1)\n",
    "df_test_checkpoint = pd.concat([df_test_checkpoint, text_embedded_w2f_df_test], axis=1)\n",
    "\n",
    "df_checkpoint.dropna(inplace=True)\n",
    "df_test_checkpoint.dropna(inplace=True)\n",
    "\n",
    "# extract y_train and y_test here to avoid column name collision with 'target' feature coming from text and keyword embeddings\n",
    "y_train = df_checkpoint['target']\n",
    "y_test = df_test_checkpoint['target']\n",
    "\n",
    "X_train = df_checkpoint.drop(['target'], axis=1)\n",
    "X_test = df_test_checkpoint.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(y_pred, y_train, y_pred_test, y_test):\n",
    "    print(\"Train results\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Train accuracy: {}\".format(accuracy_score(y_train, y_pred)))\n",
    "    print(classification_report(y_train, y_pred))\n",
    "    print(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "    print()\n",
    "    print(\"Test results\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Test accuracy: {}\".format(accuracy_score(y_test, y_pred_test)))\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results\n",
      "-----------------------------\n",
      "Train accuracy: 0.8429710022045108\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88      3595\n",
      "           1       0.86      0.72      0.78      2302\n",
      "\n",
      "    accuracy                           0.84      5897\n",
      "   macro avg       0.85      0.82      0.83      5897\n",
      "weighted avg       0.84      0.84      0.84      5897\n",
      "\n",
      "[[3319  276]\n",
      " [ 650 1652]]\n",
      "\n",
      "Test results\n",
      "-----------------------------\n",
      "Test accuracy: 0.7564338235294118\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.75      0.78      1219\n",
      "           1       0.71      0.77      0.73       957\n",
      "\n",
      "    accuracy                           0.76      2176\n",
      "   macro avg       0.75      0.76      0.75      2176\n",
      "weighted avg       0.76      0.76      0.76      2176\n",
      "\n",
      "[[913 306]\n",
      " [224 733]]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(random_state=42, solver=\"liblinear\")\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = cross_val_predict(logreg, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "y_pred_test = logreg.predict(X_test)\n",
    "\n",
    "print_results(y_pred, y_train, y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results\n",
      "-----------------------------\n",
      "Train accuracy: 0.8431405799559097\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88      3595\n",
      "           1       0.87      0.71      0.78      2302\n",
      "\n",
      "    accuracy                           0.84      5897\n",
      "   macro avg       0.85      0.82      0.83      5897\n",
      "weighted avg       0.85      0.84      0.84      5897\n",
      "\n",
      "[[3349  246]\n",
      " [ 679 1623]]\n",
      "\n",
      "Test results\n",
      "-----------------------------\n",
      "Test accuracy: 0.7568933823529411\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.74      0.77      1219\n",
      "           1       0.70      0.78      0.74       957\n",
      "\n",
      "    accuracy                           0.76      2176\n",
      "   macro avg       0.76      0.76      0.76      2176\n",
      "weighted avg       0.76      0.76      0.76      2176\n",
      "\n",
      "[[897 322]\n",
      " [207 750]]\n"
     ]
    }
   ],
   "source": [
    "if with_svm:\n",
    "  first_n = 1000\n",
    "\n",
    "  # Initialize SVM model\n",
    "  svm_model = SVC(kernel='linear', C=1, random_state=42, probability=False)\n",
    "\n",
    "  # Fit the model on training data\n",
    "  svm_model.fit(X_train, y_train)\n",
    "\n",
    "  # Use 5-fold cross-validation to get predictions on training set\n",
    "  y_pred_train = cross_val_predict(svm_model, X_train, y_train, cv=5)\n",
    "  y_pred_test = svm_model.predict(X_test)\n",
    "\n",
    "  print_results(y_pred_train, y_train, y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/magnusrodseth/dev/ntnu/h2023/it3212/notebook-new_advanced.ipynb Cell 37\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/magnusrodseth/dev/ntnu/h2023/it3212/notebook-new_advanced.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBClassifier()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/magnusrodseth/dev/ntnu/h2023/it3212/notebook-new_advanced.ipynb#X51sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#Training the model on the training data\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/magnusrodseth/dev/ntnu/h2023/it3212/notebook-new_advanced.ipynb#X51sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/magnusrodseth/dev/ntnu/h2023/it3212/notebook-new_advanced.ipynb#X51sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#Making predictions on the test set\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/magnusrodseth/dev/ntnu/h2023/it3212/notebook-new_advanced.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1462\u001b[0m (\n\u001b[1;32m   1463\u001b[0m     model,\n\u001b[1;32m   1464\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1470\u001b[0m )\n\u001b[1;32m   1471\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1472\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   1473\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1487\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[1;32m   1488\u001b[0m )\n\u001b[0;32m-> 1490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1491\u001b[0m     params,\n\u001b[1;32m   1492\u001b[0m     train_dmatrix,\n\u001b[1;32m   1493\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_num_boosting_rounds(),\n\u001b[1;32m   1494\u001b[0m     evals\u001b[39m=\u001b[39mevals,\n\u001b[1;32m   1495\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39mearly_stopping_rounds,\n\u001b[1;32m   1496\u001b[0m     evals_result\u001b[39m=\u001b[39mevals_result,\n\u001b[1;32m   1497\u001b[0m     obj\u001b[39m=\u001b[39mobj,\n\u001b[1;32m   1498\u001b[0m     custom_metric\u001b[39m=\u001b[39mmetric,\n\u001b[1;32m   1499\u001b[0m     verbose_eval\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m   1500\u001b[0m     xgb_model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m   1501\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m   1502\u001b[0m )\n\u001b[1;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[1;32m   1505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[1;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39mc_int(iteration),\n\u001b[1;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39mhandle))\n\u001b[1;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#Creating an XGBoost classifier\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "#Training the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Making predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "#Calculating accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7449448529411765\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.82      0.78      1219\n",
      "           1       0.74      0.64      0.69       957\n",
      "\n",
      "    accuracy                           0.74      2176\n",
      "   macro avg       0.74      0.73      0.74      2176\n",
      "weighted avg       0.74      0.74      0.74      2176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Adaboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "y_pred = adaboost.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
